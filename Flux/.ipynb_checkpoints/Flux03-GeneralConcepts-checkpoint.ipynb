{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Recurrent Models with FluxML</span>\n",
    "\n",
    "#### <span style=\"color:orange\"> Flux does offer out of the box a set of recurrence functionalities in specific layers. Remember that recurrent models can come across as being more complicated than necessary. In general we are still dealing with the same type of funcational relationship, $\\hat{y} = f(X_i) = f_{rnn}(X_t)$ where previously y_hat was either a single dimension or multiple dimensions, here $y_{hat} = [y_t , h_t]$ where $h_t$ is an input into the new time point (memory carry on) so that we have $X_t = [ x_t , h_{t-1}]$. The dependency can be seen as $\\hat{y}_t = f(x_t,h_{t-1}), h_t = g(h_{t-1},x_t)$ </span>\n",
    "\n",
    "- This basic recurrence relationship says that at each point we take the $h_{t-1}$ from the previous time step 't-1', we also use the current inputs at time t, $x_t$ and then produce an output which is has 2 components, $y_t$ and $h_t$ (where $h_t$ feeds into $t+1$). If we focus on a single time point, we are still doing the functional mapping that we had before with the chain of Dense layers.\n",
    "\n",
    "- The below image [link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fswlh%2Fintroduction-to-recurrent-neural-networks-rnns-347903dd8d81&psig=AOvVaw3xmMdMdDizNUUWXy021QUO&ust=1673451409556000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCOCStLiqvfwCFQAAAAAdAAAAABA_) shows how this looks in a model diagram for **seq-to-seq** (**many to many**)\n",
    "\n",
    "![rnn](./rnn1.png)\n",
    "\n",
    "- also The below image [link source](https://www.ibm.com/topics/recurrent-neural-networks) shows how this looks\n",
    "\n",
    "![rnn](./rnn2.jpeg)\n",
    "\n",
    "----------------\n",
    "\n",
    "#### <span style=\"color:orange\"> $W_h$ is the weight matrix (tranformation) on the 'hidden inputs' $h_{t-1}$ that come from the previous unit's 'hidden' output, $W_x$ is the weight matrix (transformation) upon the inputs at the current time $x_t$. $W_y$ is the transformation weight matrix applied to the 'current' hidden value produced from the cell that after a non-linear transformation (activation function) produces the output $\\hat{y}_t$. Training involves learning the values of the weights/parameters for these matrices.</span>\n",
    "\n",
    "### <span style=\"color:orange\"> $h_t = tahn(b_h + W_h^t h_{t-1} + W_x^t x_t)$ </span>\n",
    "### <span style=\"color:orange\"> $\\hat{y}_t = softmax(b_y + W_{y}^{t} h_t)$ </span>\n",
    "\n",
    "RNN can bring to mind the Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/.julia/environments/v1.8/Project.toml`\n",
      " \u001b[90m [587475ba] \u001b[39mFlux v0.13.11\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.status(\"Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using Plots\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[-0.42449322, -0.780341, 0.8858426, 0.3879123, 0.64506674]\n"
     ]
    }
   ],
   "source": [
    "#example of usage of the RNN unit\n",
    "h_dim = 5 #hidden dimension\n",
    "x_dim = 2 #input dimension at time t\n",
    "y_dim = 1 #the output dimension that 'we' see\n",
    "\n",
    "rnn_tmp = RNN( x_dim , h_dim ) #produces the cell\n",
    "x_t1 = Float32.( [1,2] ) #some arbitrary input data\n",
    "println( rnn_tmp( x_t1 ) ) #print the output h_t1 from the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> It is key to know that this cell is different from the Dense layers in that it maintains the state between executions since it is **stateful**. This means it holds the state via a closure inside the function reference</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[-0.88633436, -0.9421285, 0.93829435, -0.2912246, -0.7543197]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Vector{Float32}}:\n",
       " [-0.94388855, -0.88409203, 0.7260095, 0.33946013, -0.37083846]\n",
       " [-0.94723046, -0.8331355, 0.7455543, -0.013459079, -0.5831472]\n",
       " [-0.9384129, -0.82715374, 0.7115134, 0.18569449, -0.39269543]\n",
       " [-0.9388833, -0.8252977, 0.74333215, 0.047026403, -0.50348234]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_t1 = Float32.( [1,2] ) #some arbitrary input data\n",
    "println( rnn_tmp( x_t1 ) ) #print the output h_t1 from the cell\n",
    "#print multiple times to see the changes\n",
    "display( [ rnn_tmp( x_t1 ) for _ in 1:4 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> Since the RNN unit maintains and handles the state between subsequent uses we can abstractly use it in the ML pipeline as we used the Dense layer. From above notice that we produced hidden representation responses from inputs, but not the predictions $\\hat{y}$ since those are done separately. </span>\n",
    "\n",
    "### <span style=\"color:orange\"> The RNN function implements $h_t = tahn(b_h + W_h^t h_{t-1} + W_x^t x_t)$ but $\\hat{y}_t = softmax(b_y + W_{y}^{t} h_t)$ is not. The $W_{y}$ matrix is not provided by the RNN layer and must be supplied by the user. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Recur(\n",
       "    RNNCell(2 => 5, tanh),              \u001b[90m# 45 parameters\u001b[39m\n",
       "  ),\n",
       "  Dense(5 => 1, relu),                  \u001b[90m# 6 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m        # Total: 6 trainable arrays, \u001b[39m51 parameters,\n",
       "\u001b[90m          # plus 1 non-trainable, 5 parameters, summarysize \u001b[39m580 bytes."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we feed the model with 'x_dim' data, it produces a hidden vector 'h_dim' and outputs a 'y_dim' vector at each time\n",
    "rnn_model1 = Chain( RNN( x_dim => h_dim ) , Dense( h_dim => y_dim , relu ), softmax ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 1.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try out the model\n",
    "rnn_model1( x_t1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Vector{Float32}}:\n",
       " [1.0]\n",
       " [1.0]\n",
       " [1.0]\n",
       " [1.0]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ rnn_model1( x_t1 ) for _ in 1:4 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Vector{Float32}}:\n",
       " [1.0]\n",
       " [1.0]\n",
       " [1.0]\n",
       " [1.0]\n",
       " [1.0]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the model with hypothetical data\n",
    "x_length = 5\n",
    "#generate some random data as inputs, to be treated as a sequence\n",
    "x_seq = [ rand( Float32 , x_dim ) for i = 1:x_length ] #sequence data\n",
    "[ rnn_model1( xt ) for xt in x_seq ] #predicted y_t data from the RNN\n",
    "#this is <sequence to sequence> <many to many>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us look at how we can use batches, where a batch is a set of independent sentences in a single datastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [(2, 4), (2, 4), (2, 4), (2, 4), (2, 4)]\n",
      "y [(1, 4), (1, 4), (1, 4), (1, 4), (1, 4)]\n",
      "loss1 1.9478507\n",
      "loss2 1.9478507\n"
     ]
    }
   ],
   "source": [
    "batch_num = 4\n",
    "x_seq = [ rand( Float32 , x_dim , batch_num ) for i = 1:x_length ]\n",
    "display( size( x_seq[1] ) )\n",
    "y_seq = [ rand( Float32 , y_dim , batch_num ) for i = 1:x_length ]\n",
    "display( size( y[1] ) )\n",
    "\n",
    "function loss1(x, y)\n",
    "    Flux.reset!(m)\n",
    "    println( \"x \" , [ size(xi) for (xi,yi) in zip(x,y) ] )\n",
    "    println( \"y \" , [ size(yi) for (xi,yi) in zip(x,y) ] )\n",
    "    #the way Julia does broadcasting means that we don't need to reset between batches\n",
    "    return sum( [ Flux.mse(rnn_model1(x_b),y_b) for (x_b,y_b) in zip(x,y) ] )\n",
    "end\n",
    "println( \"loss1 \",loss1(x_seq,y_seq) )\n",
    "\n",
    "function loss2(x, y)\n",
    "    Flux.reset!(m)\n",
    "    loss = 0\n",
    "    for (x_b,y_b) in zip(x,y)\n",
    "        loss += Flux.mse(rnn_model1(x_b),y_b)\n",
    "    end\n",
    "    return loss\n",
    "end\n",
    "println( \"loss2 \",loss2(x_seq,y_seq) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> If you need to use the RNN and not have it dependent upon the previous state (eg. independent sentences), then you can use the **Flux.reset!(rnn_model)** command so the previous history variables are removed </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Recur(\n",
       "    RNNCell(2 => 4, tanh),              \u001b[90m# 32 parameters\u001b[39m\n",
       "  ),\n",
       "  Dense(4 => 3, relu),                  \u001b[90m# 15 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m        # Total: 6 trainable arrays, \u001b[39m47 parameters,\n",
       "\u001b[90m          # plus 1 non-trainable, 4 parameters, summarysize \u001b[39m564 bytes."
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start a new model fresh again\n",
    "x_categories = [ \"happy\" , \"sad\" ]\n",
    "y_categories = [ \"steak\" , \"chicken\" , \"soup\" ]\n",
    "h_dim = 4 #hidden dimension\n",
    "x_dim = length( x_categories ) #input dimension at time t\n",
    "y_dim = length( y_categories ) #the output dimension that 'we' see\n",
    "sequence_length = 4\n",
    "p_emotions = [0.6,0.4] #higher prob of happy\n",
    "\n",
    "rnn_model2 = Chain( RNN( x_dim => h_dim ) , Dense( h_dim => y_dim,relu), softmax )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.32682362, 0.34635276, 0.32682362]\n",
      "Float32[0.32682362, 0.34635276, 0.32682362]\n",
      "Float32[0.32682362, 0.34635276, 0.32682362]\n",
      "Float32[0.32682362, 0.34635276, 0.32682362]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"----updating (no reset)----\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.31895843, 0.31895843, 0.36208317]\n",
      "Float32[0.30174065, 0.30174065, 0.39651868]\n",
      "Float32[0.30505842, 0.30505842, 0.38988313]\n",
      "Float32[0.30395216, 0.30395216, 0.39209569]\n"
     ]
    }
   ],
   "source": [
    "#no updating via reset which resets the RNN hidden state\n",
    "x_tmp = rand( Float32 , x_dim )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "#updating\n",
    "display( \"----updating (no reset)----\" )\n",
    "println( rnn_model2(x_tmp) )\n",
    "println( rnn_model2(x_tmp) )\n",
    "println( rnn_model2(x_tmp) )\n",
    "println( rnn_model2(x_tmp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (make some data on food choices) dependency is that when happy is seen: steak, then on subsequent happy: chicken, upon sad: soup (constantly)\n",
    "\n",
    "the key is here that there should be memory of having seen sad which overrides newer observations of happy so that the menu serves soup regardless and that the RNN stores this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sad\", \"sad\", \"happy\", \"sad\"]\n",
      "Any[\"soup\", \"soup\", \"steak\", \"soup\"]\n",
      "-------------\n",
      "[\"happy\", \"sad\", \"happy\", \"happy\"]\n",
      "Any[\"steak\", \"soup\", \"steak\", \"chicken\"]\n"
     ]
    }
   ],
   "source": [
    "function RNN_X_Data_Food( sequence_length )\n",
    "    #rand(x_categories,sequence_length)\n",
    "    return x_categories[ rand(Categorical(p_emotions),sequence_length) ] \n",
    "end\n",
    "x_seq_cold = RNN_X_Data_Food( sequence_length )\n",
    "println( x_seq_cold )\n",
    "#dependency is that when happy is seen: steak, then on subsequent happy: chicken, upon sad: soup (constantly)\n",
    "function RNN_Y_Data_Food( x_seq_cold )\n",
    "    y_seq_cold = []\n",
    "    prev_xx = \"\"\n",
    "    seen_sad = false\n",
    "    for xx in x_seq_cold\n",
    "        if( seen_sad == false && prev_xx != \"happy\" && xx == \"happy\" )\n",
    "            push!( y_seq_cold , \"steak\" )\n",
    "        elseif( seen_sad == false && prev_xx == \"happy\" && xx == \"happy\" )\n",
    "            push!( y_seq_cold , \"chicken\" )\n",
    "        else\n",
    "            push!( y_seq_cold , \"soup\" )\n",
    "            #seen_sad = true\n",
    "        end\n",
    "        prev_xx = xx\n",
    "    end\n",
    "    return y_seq_cold\n",
    "end\n",
    "println( RNN_Y_Data_Food(x_seq_cold) ) #output food agenda\n",
    "println(\"-------------\")\n",
    "x_seq_cold = RNN_X_Data_Food( sequence_length )\n",
    "println( x_seq_cold )\n",
    "println( RNN_Y_Data_Food(x_seq_cold) ) #output food agenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Vector{String}}:\n",
       " [\"sad\", \"sad\", \"sad\", \"happy\"]\n",
       " [\"sad\", \"sad\", \"happy\", \"sad\"]\n",
       " [\"happy\", \"sad\", \"happy\", \"happy\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Vector{Vector{Any}}:\n",
       " [\"soup\", \"soup\", \"soup\", \"steak\"]\n",
       " [\"soup\", \"soup\", \"steak\", \"soup\"]\n",
       " [\"steak\", \"soup\", \"steak\", \"chicken\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN = 10\n",
    "#generate the stochastic data upon the rule set defined in the functions\n",
    "x_train_cold = [ RNN_X_Data_Food(sequence_length) for _ in 1:NN ]\n",
    "display( x_train_cold[1:3] )\n",
    "y_train_cold = RNN_Y_Data_Food.(x_train_cold)\n",
    "display( y_train_cold[1:3] )\n",
    "x_test_cold = [ RNN_X_Data_Food(sequence_length) for _ in 1:NN ]\n",
    "y_test_cold = RNN_Y_Data_Food.(x_test_cold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{OneHotArrays.OneHotMatrix{UInt32, 2, Vector{UInt32}}}:\n",
       " [0 0 0 1; 1 1 1 0]\n",
       " [0 0 1 0; 1 1 0 1]\n",
       " [1 0 1 1; 0 1 0 0]\n",
       " [1 1 0 1; 0 0 1 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{OneHotArrays.OneHotMatrix{UInt32, 3, Vector{UInt32}}}:\n",
       " [0 0 0 1; 0 0 0 0; 1 1 1 0]\n",
       " [0 0 1 0; 0 0 0 0; 1 1 0 1]\n",
       " [1 0 1 0; 0 0 0 1; 0 1 0 0]\n",
       " [1 0 0 1; 0 1 0 0; 0 0 1 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#put the label data into one-hot-batch\n",
    "x_train = [ Flux.onehotbatch( x_train_cold[ii] , x_categories ) for ii in 1:length(x_train_cold) ]\n",
    "display( x_train[1:4] )\n",
    "x_test = [ Flux.onehotbatch( x_test_cold[ii] , x_categories ) for ii in 1:length(x_test_cold) ];\n",
    "y_train = [ Flux.onehotbatch( y_train_cold[ii] , y_categories ) for ii in 1:length(y_train_cold) ];\n",
    "display( y_train[1:4] )\n",
    "y_test = [ Flux.onehotbatch( y_test_cold[ii] , y_categories ) for ii in 1:length(y_test_cold) ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sad\", \"sad\", \"sad\", \"happy\"]\n",
      "Any[\"soup\", \"soup\", \"soup\", \"steak\"]\n",
      "Bool[0 0 0 1; 1 1 1 0]\n",
      "Bool[0 0 0 1; 0 0 0 0; 1 1 1 0]\n",
      "Float32[0.30488 0.30488 0.30488 0.3375943; 0.29354772 0.29354772 0.29354772 0.30994758; 0.40157223 0.40157223 0.40157223 0.3524581]\n",
      "[\"soup\", \"soup\", \"soup\", \"soup\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18923242f0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println( x_train_cold[1] ) #labels input\n",
    "println( y_train_cold[1] ) #food output\n",
    "println( x_train[1] ) #in numerical 1hot\n",
    "println( y_train[1] ) #in numerical 1hot\n",
    "\n",
    "y_hat = rnn_model2( x_train[1] ) #inspect what the model predicts given some training data (not trained yet)\n",
    "println( y_hat ) #raw prediction vectors\n",
    "println( Flux.onecold( y_hat , y_categories ) ) #the predicted \n",
    "Flux.mse( y_hat , y_train[1] ) #the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip070\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip070)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip071\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip070)\" d=\"\n",
       "M155.765 1486.45 L2352.76 1486.45 L2352.76 123.472 L155.765 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip072\">\n",
       "    <rect x=\"155\" y=\"123\" width=\"2198\" height=\"1364\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  216.907,1486.45 216.907,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  735.325,1486.45 735.325,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1253.74,1486.45 1253.74,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1772.16,1486.45 1772.16,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.58,1486.45 2290.58,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  155.765,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  216.907,1486.45 216.907,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  735.325,1486.45 735.325,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1253.74,1486.45 1253.74,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1772.16,1486.45 1772.16,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.58,1486.45 2290.58,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M216.907 1517.37 Q213.296 1517.37 211.467 1520.93 Q209.662 1524.47 209.662 1531.6 Q209.662 1538.71 211.467 1542.27 Q213.296 1545.82 216.907 1545.82 Q220.541 1545.82 222.347 1542.27 Q224.176 1538.71 224.176 1531.6 Q224.176 1524.47 222.347 1520.93 Q220.541 1517.37 216.907 1517.37 M216.907 1513.66 Q222.717 1513.66 225.773 1518.27 Q228.851 1522.85 228.851 1531.6 Q228.851 1540.33 225.773 1544.94 Q222.717 1549.52 216.907 1549.52 Q211.097 1549.52 208.018 1544.94 Q204.963 1540.33 204.963 1531.6 Q204.963 1522.85 208.018 1518.27 Q211.097 1513.66 216.907 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M694.943 1514.29 L713.299 1514.29 L713.299 1518.22 L699.225 1518.22 L699.225 1526.7 Q700.244 1526.35 701.262 1526.19 Q702.281 1526 703.299 1526 Q709.086 1526 712.466 1529.17 Q715.845 1532.34 715.845 1537.76 Q715.845 1543.34 712.373 1546.44 Q708.901 1549.52 702.582 1549.52 Q700.406 1549.52 698.137 1549.15 Q695.892 1548.78 693.484 1548.04 L693.484 1543.34 Q695.568 1544.47 697.79 1545.03 Q700.012 1545.58 702.489 1545.58 Q706.494 1545.58 708.832 1543.48 Q711.17 1541.37 711.17 1537.76 Q711.17 1534.15 708.832 1532.04 Q706.494 1529.94 702.489 1529.94 Q700.614 1529.94 698.739 1530.35 Q696.887 1530.77 694.943 1531.65 L694.943 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M735.058 1517.37 Q731.447 1517.37 729.619 1520.93 Q727.813 1524.47 727.813 1531.6 Q727.813 1538.71 729.619 1542.27 Q731.447 1545.82 735.058 1545.82 Q738.693 1545.82 740.498 1542.27 Q742.327 1538.71 742.327 1531.6 Q742.327 1524.47 740.498 1520.93 Q738.693 1517.37 735.058 1517.37 M735.058 1513.66 Q740.868 1513.66 743.924 1518.27 Q747.003 1522.85 747.003 1531.6 Q747.003 1540.33 743.924 1544.94 Q740.868 1549.52 735.058 1549.52 Q729.248 1549.52 726.169 1544.94 Q723.114 1540.33 723.114 1531.6 Q723.114 1522.85 726.169 1518.27 Q729.248 1513.66 735.058 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M765.22 1517.37 Q761.609 1517.37 759.78 1520.93 Q757.975 1524.47 757.975 1531.6 Q757.975 1538.71 759.78 1542.27 Q761.609 1545.82 765.22 1545.82 Q768.854 1545.82 770.66 1542.27 Q772.489 1538.71 772.489 1531.6 Q772.489 1524.47 770.66 1520.93 Q768.854 1517.37 765.22 1517.37 M765.22 1513.66 Q771.03 1513.66 774.086 1518.27 Q777.165 1522.85 777.165 1531.6 Q777.165 1540.33 774.086 1544.94 Q771.03 1549.52 765.22 1549.52 Q759.41 1549.52 756.331 1544.94 Q753.276 1540.33 753.276 1531.6 Q753.276 1522.85 756.331 1518.27 Q759.41 1513.66 765.22 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1198.27 1544.91 L1205.91 1544.91 L1205.91 1518.55 L1197.6 1520.21 L1197.6 1515.95 L1205.86 1514.29 L1210.54 1514.29 L1210.54 1544.91 L1218.18 1544.91 L1218.18 1548.85 L1198.27 1548.85 L1198.27 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1237.62 1517.37 Q1234.01 1517.37 1232.18 1520.93 Q1230.37 1524.47 1230.37 1531.6 Q1230.37 1538.71 1232.18 1542.27 Q1234.01 1545.82 1237.62 1545.82 Q1241.25 1545.82 1243.06 1542.27 Q1244.89 1538.71 1244.89 1531.6 Q1244.89 1524.47 1243.06 1520.93 Q1241.25 1517.37 1237.62 1517.37 M1237.62 1513.66 Q1243.43 1513.66 1246.49 1518.27 Q1249.56 1522.85 1249.56 1531.6 Q1249.56 1540.33 1246.49 1544.94 Q1243.43 1549.52 1237.62 1549.52 Q1231.81 1549.52 1228.73 1544.94 Q1225.68 1540.33 1225.68 1531.6 Q1225.68 1522.85 1228.73 1518.27 Q1231.81 1513.66 1237.62 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1267.78 1517.37 Q1264.17 1517.37 1262.34 1520.93 Q1260.54 1524.47 1260.54 1531.6 Q1260.54 1538.71 1262.34 1542.27 Q1264.17 1545.82 1267.78 1545.82 Q1271.42 1545.82 1273.22 1542.27 Q1275.05 1538.71 1275.05 1531.6 Q1275.05 1524.47 1273.22 1520.93 Q1271.42 1517.37 1267.78 1517.37 M1267.78 1513.66 Q1273.59 1513.66 1276.65 1518.27 Q1279.73 1522.85 1279.73 1531.6 Q1279.73 1540.33 1276.65 1544.94 Q1273.59 1549.52 1267.78 1549.52 Q1261.97 1549.52 1258.89 1544.94 Q1255.84 1540.33 1255.84 1531.6 Q1255.84 1522.85 1258.89 1518.27 Q1261.97 1513.66 1267.78 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1297.94 1517.37 Q1294.33 1517.37 1292.5 1520.93 Q1290.7 1524.47 1290.7 1531.6 Q1290.7 1538.71 1292.5 1542.27 Q1294.33 1545.82 1297.94 1545.82 Q1301.58 1545.82 1303.38 1542.27 Q1305.21 1538.71 1305.21 1531.6 Q1305.21 1524.47 1303.38 1520.93 Q1301.58 1517.37 1297.94 1517.37 M1297.94 1513.66 Q1303.75 1513.66 1306.81 1518.27 Q1309.89 1522.85 1309.89 1531.6 Q1309.89 1540.33 1306.81 1544.94 Q1303.75 1549.52 1297.94 1549.52 Q1292.13 1549.52 1289.05 1544.94 Q1286 1540.33 1286 1531.6 Q1286 1522.85 1289.05 1518.27 Q1292.13 1513.66 1297.94 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1716.69 1544.91 L1724.32 1544.91 L1724.32 1518.55 L1716.01 1520.21 L1716.01 1515.95 L1724.28 1514.29 L1728.95 1514.29 L1728.95 1544.91 L1736.59 1544.91 L1736.59 1548.85 L1716.69 1548.85 L1716.69 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1746.08 1514.29 L1764.44 1514.29 L1764.44 1518.22 L1750.37 1518.22 L1750.37 1526.7 Q1751.38 1526.35 1752.4 1526.19 Q1753.42 1526 1754.44 1526 Q1760.23 1526 1763.61 1529.17 Q1766.99 1532.34 1766.99 1537.76 Q1766.99 1543.34 1763.51 1546.44 Q1760.04 1549.52 1753.72 1549.52 Q1751.55 1549.52 1749.28 1549.15 Q1747.03 1548.78 1744.62 1548.04 L1744.62 1543.34 Q1746.71 1544.47 1748.93 1545.03 Q1751.15 1545.58 1753.63 1545.58 Q1757.63 1545.58 1759.97 1543.48 Q1762.31 1541.37 1762.31 1537.76 Q1762.31 1534.15 1759.97 1532.04 Q1757.63 1529.94 1753.63 1529.94 Q1751.75 1529.94 1749.88 1530.35 Q1748.03 1530.77 1746.08 1531.65 L1746.08 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1786.2 1517.37 Q1782.59 1517.37 1780.76 1520.93 Q1778.95 1524.47 1778.95 1531.6 Q1778.95 1538.71 1780.76 1542.27 Q1782.59 1545.82 1786.2 1545.82 Q1789.83 1545.82 1791.64 1542.27 Q1793.47 1538.71 1793.47 1531.6 Q1793.47 1524.47 1791.64 1520.93 Q1789.83 1517.37 1786.2 1517.37 M1786.2 1513.66 Q1792.01 1513.66 1795.06 1518.27 Q1798.14 1522.85 1798.14 1531.6 Q1798.14 1540.33 1795.06 1544.94 Q1792.01 1549.52 1786.2 1549.52 Q1780.39 1549.52 1777.31 1544.94 Q1774.25 1540.33 1774.25 1531.6 Q1774.25 1522.85 1777.31 1518.27 Q1780.39 1513.66 1786.2 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1816.36 1517.37 Q1812.75 1517.37 1810.92 1520.93 Q1809.12 1524.47 1809.12 1531.6 Q1809.12 1538.71 1810.92 1542.27 Q1812.75 1545.82 1816.36 1545.82 Q1819.99 1545.82 1821.8 1542.27 Q1823.63 1538.71 1823.63 1531.6 Q1823.63 1524.47 1821.8 1520.93 Q1819.99 1517.37 1816.36 1517.37 M1816.36 1513.66 Q1822.17 1513.66 1825.23 1518.27 Q1828.3 1522.85 1828.3 1531.6 Q1828.3 1540.33 1825.23 1544.94 Q1822.17 1549.52 1816.36 1549.52 Q1810.55 1549.52 1807.47 1544.94 Q1804.42 1540.33 1804.42 1531.6 Q1804.42 1522.85 1807.47 1518.27 Q1810.55 1513.66 1816.36 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2239.19 1544.91 L2255.51 1544.91 L2255.51 1548.85 L2233.56 1548.85 L2233.56 1544.91 Q2236.23 1542.16 2240.81 1537.53 Q2245.42 1532.88 2246.6 1531.53 Q2248.84 1529.01 2249.72 1527.27 Q2250.62 1525.51 2250.62 1523.82 Q2250.62 1521.07 2248.68 1519.33 Q2246.76 1517.6 2243.66 1517.6 Q2241.46 1517.6 2239 1518.36 Q2236.57 1519.13 2233.79 1520.68 L2233.79 1515.95 Q2236.62 1514.82 2239.07 1514.24 Q2241.53 1513.66 2243.56 1513.66 Q2248.93 1513.66 2252.13 1516.35 Q2255.32 1519.03 2255.32 1523.52 Q2255.32 1525.65 2254.51 1527.57 Q2253.73 1529.47 2251.62 1532.07 Q2251.04 1532.74 2247.94 1535.95 Q2244.84 1539.15 2239.19 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2275.32 1517.37 Q2271.71 1517.37 2269.88 1520.93 Q2268.08 1524.47 2268.08 1531.6 Q2268.08 1538.71 2269.88 1542.27 Q2271.71 1545.82 2275.32 1545.82 Q2278.96 1545.82 2280.76 1542.27 Q2282.59 1538.71 2282.59 1531.6 Q2282.59 1524.47 2280.76 1520.93 Q2278.96 1517.37 2275.32 1517.37 M2275.32 1513.66 Q2281.13 1513.66 2284.19 1518.27 Q2287.27 1522.85 2287.27 1531.6 Q2287.27 1540.33 2284.19 1544.94 Q2281.13 1549.52 2275.32 1549.52 Q2269.51 1549.52 2266.43 1544.94 Q2263.38 1540.33 2263.38 1531.6 Q2263.38 1522.85 2266.43 1518.27 Q2269.51 1513.66 2275.32 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2305.48 1517.37 Q2301.87 1517.37 2300.04 1520.93 Q2298.24 1524.47 2298.24 1531.6 Q2298.24 1538.71 2300.04 1542.27 Q2301.87 1545.82 2305.48 1545.82 Q2309.12 1545.82 2310.92 1542.27 Q2312.75 1538.71 2312.75 1531.6 Q2312.75 1524.47 2310.92 1520.93 Q2309.12 1517.37 2305.48 1517.37 M2305.48 1513.66 Q2311.29 1513.66 2314.35 1518.27 Q2317.43 1522.85 2317.43 1531.6 Q2317.43 1540.33 2314.35 1544.94 Q2311.29 1549.52 2305.48 1549.52 Q2299.67 1549.52 2296.6 1544.94 Q2293.54 1540.33 2293.54 1531.6 Q2293.54 1522.85 2296.6 1518.27 Q2299.67 1513.66 2305.48 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M2335.65 1517.37 Q2332.04 1517.37 2330.21 1520.93 Q2328.4 1524.47 2328.4 1531.6 Q2328.4 1538.71 2330.21 1542.27 Q2332.04 1545.82 2335.65 1545.82 Q2339.28 1545.82 2341.09 1542.27 Q2342.91 1538.71 2342.91 1531.6 Q2342.91 1524.47 2341.09 1520.93 Q2339.28 1517.37 2335.65 1517.37 M2335.65 1513.66 Q2341.46 1513.66 2344.51 1518.27 Q2347.59 1522.85 2347.59 1531.6 Q2347.59 1540.33 2344.51 1544.94 Q2341.46 1549.52 2335.65 1549.52 Q2329.84 1549.52 2326.76 1544.94 Q2323.7 1540.33 2323.7 1531.6 Q2323.7 1522.85 2326.76 1518.27 Q2329.84 1513.66 2335.65 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  155.765,1301.24 2352.76,1301.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  155.765,1006.28 2352.76,1006.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  155.765,711.329 2352.76,711.329 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip072)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  155.765,416.375 2352.76,416.375 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  155.765,1486.45 155.765,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  155.765,1301.24 174.663,1301.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  155.765,1006.28 174.663,1006.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  155.765,711.329 174.663,711.329 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip070)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  155.765,416.375 174.663,416.375 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip070)\" d=\"M53.2375 1314.58 L60.8763 1314.58 L60.8763 1288.22 L52.5662 1289.88 L52.5662 1285.62 L60.83 1283.96 L65.5059 1283.96 L65.5059 1314.58 L73.1448 1314.58 L73.1448 1318.52 L53.2375 1318.52 L53.2375 1314.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M82.5892 1312.64 L87.4734 1312.64 L87.4734 1318.52 L82.5892 1318.52 L82.5892 1312.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M108.237 1299.37 Q105.089 1299.37 103.237 1301.53 Q101.409 1303.68 101.409 1307.43 Q101.409 1311.16 103.237 1313.33 Q105.089 1315.48 108.237 1315.48 Q111.385 1315.48 113.214 1313.33 Q115.066 1311.16 115.066 1307.43 Q115.066 1303.68 113.214 1301.53 Q111.385 1299.37 108.237 1299.37 M117.52 1284.72 L117.52 1288.98 Q115.76 1288.15 113.955 1287.71 Q112.172 1287.27 110.413 1287.27 Q105.783 1287.27 103.33 1290.39 Q100.899 1293.52 100.552 1299.84 Q101.918 1297.82 103.978 1296.76 Q106.038 1295.67 108.515 1295.67 Q113.723 1295.67 116.733 1298.84 Q119.765 1301.99 119.765 1307.43 Q119.765 1312.75 116.617 1315.97 Q113.469 1319.19 108.237 1319.19 Q102.242 1319.19 99.0706 1314.6 Q95.8993 1310 95.8993 1301.27 Q95.8993 1293.08 99.7882 1288.22 Q103.677 1283.33 110.228 1283.33 Q111.987 1283.33 113.77 1283.68 Q115.575 1284.03 117.52 1284.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M53.4921 1019.63 L61.131 1019.63 L61.131 993.262 L52.8208 994.929 L52.8208 990.669 L61.0847 989.003 L65.7606 989.003 L65.7606 1019.63 L73.3994 1019.63 L73.3994 1023.56 L53.4921 1023.56 L53.4921 1019.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M82.8438 1017.68 L87.728 1017.68 L87.728 1023.56 L82.8438 1023.56 L82.8438 1017.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M107.913 1007.15 Q104.58 1007.15 102.659 1008.93 Q100.76 1010.72 100.76 1013.84 Q100.76 1016.97 102.659 1018.75 Q104.58 1020.53 107.913 1020.53 Q111.246 1020.53 113.168 1018.75 Q115.089 1016.94 115.089 1013.84 Q115.089 1010.72 113.168 1008.93 Q111.27 1007.15 107.913 1007.15 M103.237 1005.16 Q100.228 1004.42 98.5382 1002.36 Q96.8715 1000.3 96.8715 997.336 Q96.8715 993.193 99.8113 990.785 Q102.774 988.378 107.913 988.378 Q113.075 988.378 116.015 990.785 Q118.955 993.193 118.955 997.336 Q118.955 1000.3 117.265 1002.36 Q115.598 1004.42 112.612 1005.16 Q115.992 1005.95 117.867 1008.24 Q119.765 1010.53 119.765 1013.84 Q119.765 1018.86 116.686 1021.55 Q113.631 1024.23 107.913 1024.23 Q102.196 1024.23 99.1169 1021.55 Q96.0613 1018.86 96.0613 1013.84 Q96.0613 1010.53 97.9595 1008.24 Q99.8576 1005.95 103.237 1005.16 M101.524 997.776 Q101.524 1000.46 103.191 1001.97 Q104.881 1003.47 107.913 1003.47 Q110.922 1003.47 112.612 1001.97 Q114.325 1000.46 114.325 997.776 Q114.325 995.091 112.612 993.586 Q110.922 992.081 107.913 992.081 Q104.881 992.081 103.191 993.586 Q101.524 995.091 101.524 997.776 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M56.6171 724.674 L72.9365 724.674 L72.9365 728.609 L50.9921 728.609 L50.9921 724.674 Q53.6541 721.919 58.2375 717.289 Q62.8439 712.637 64.0245 711.294 Q66.2698 708.771 67.1494 707.035 Q68.0522 705.276 68.0522 703.586 Q68.0522 700.831 66.1078 699.095 Q64.1865 697.359 61.0847 697.359 Q58.8856 697.359 56.4319 698.123 Q54.0014 698.887 51.2236 700.438 L51.2236 695.716 Q54.0477 694.581 56.5014 694.003 Q58.955 693.424 60.9921 693.424 Q66.3624 693.424 69.5568 696.109 Q72.7513 698.794 72.7513 703.285 Q72.7513 705.415 71.9411 707.336 Q71.1541 709.234 69.0476 711.827 Q68.4689 712.498 65.367 715.715 Q62.2652 718.91 56.6171 724.674 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M82.7512 722.729 L87.6354 722.729 L87.6354 728.609 L82.7512 728.609 L82.7512 722.729 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M107.821 697.128 Q104.209 697.128 102.381 700.692 Q100.575 704.234 100.575 711.364 Q100.575 718.47 102.381 722.035 Q104.209 725.576 107.821 725.576 Q111.455 725.576 113.26 722.035 Q115.089 718.47 115.089 711.364 Q115.089 704.234 113.26 700.692 Q111.455 697.128 107.821 697.128 M107.821 693.424 Q113.631 693.424 116.686 698.03 Q119.765 702.614 119.765 711.364 Q119.765 720.09 116.686 724.697 Q113.631 729.28 107.821 729.28 Q102.01 729.28 98.9317 724.697 Q95.8761 720.09 95.8761 711.364 Q95.8761 702.614 98.9317 698.03 Q102.01 693.424 107.821 693.424 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M58.2143 429.72 L74.5337 429.72 L74.5337 433.655 L52.5893 433.655 L52.5893 429.72 Q55.2514 426.965 59.8347 422.336 Q64.4411 417.683 65.6217 416.34 Q67.867 413.817 68.7467 412.081 Q69.6494 410.322 69.6494 408.632 Q69.6494 405.877 67.705 404.141 Q65.7837 402.405 62.6819 402.405 Q60.4828 402.405 58.0291 403.169 Q55.5986 403.933 52.8208 405.484 L52.8208 400.762 Q55.6449 399.627 58.0986 399.049 Q60.5523 398.47 62.5893 398.47 Q67.9596 398.47 71.1541 401.155 Q74.3485 403.84 74.3485 408.331 Q74.3485 410.461 73.5383 412.382 Q72.7513 414.28 70.6448 416.873 Q70.0661 417.544 66.9643 420.762 Q63.8624 423.956 58.2143 429.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M84.3484 427.775 L89.2327 427.775 L89.2327 433.655 L84.3484 433.655 L84.3484 427.775 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M103.446 429.72 L119.765 429.72 L119.765 433.655 L97.8206 433.655 L97.8206 429.72 Q100.483 426.965 105.066 422.336 Q109.672 417.683 110.853 416.34 Q113.098 413.817 113.978 412.081 Q114.881 410.322 114.881 408.632 Q114.881 405.877 112.936 404.141 Q111.015 402.405 107.913 402.405 Q105.714 402.405 103.26 403.169 Q100.83 403.933 98.0521 405.484 L98.0521 400.762 Q100.876 399.627 103.33 399.049 Q105.783 398.47 107.821 398.47 Q113.191 398.47 116.385 401.155 Q119.58 403.84 119.58 408.331 Q119.58 410.461 118.77 412.382 Q117.983 414.28 115.876 416.873 Q115.297 417.544 112.196 420.762 Q109.094 423.956 103.446 429.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M826.08 44.2197 Q828.713 45.1109 831.184 48.0275 Q833.696 50.9442 836.207 56.0483 L844.511 72.576 L835.721 72.576 L827.984 57.061 Q824.986 50.9847 822.15 48.9997 Q819.355 47.0148 814.494 47.0148 L805.582 47.0148 L805.582 72.576 L797.399 72.576 L797.399 12.096 L815.872 12.096 Q826.242 12.096 831.346 16.4305 Q836.45 20.7649 836.45 29.5149 Q836.45 35.2267 833.777 38.994 Q831.143 42.7613 826.08 44.2197 M805.582 18.8205 L805.582 40.2903 L815.872 40.2903 Q821.786 40.2903 824.784 37.5762 Q827.822 34.8216 827.822 29.5149 Q827.822 24.2082 824.784 21.5346 Q821.786 18.8205 815.872 18.8205 L805.582 18.8205 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M855.044 12.096 L866.062 12.096 L892.879 62.6918 L892.879 12.096 L900.819 12.096 L900.819 72.576 L889.801 72.576 L862.984 21.9802 L862.984 72.576 L855.044 72.576 L855.044 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M917.104 12.096 L928.122 12.096 L954.939 62.6918 L954.939 12.096 L962.879 12.096 L962.879 72.576 L951.86 72.576 L925.043 21.9802 L925.043 72.576 L917.104 72.576 L917.104 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1028.18 9.54393 L1028.18 15.7418 L1021.05 15.7418 Q1017.04 15.7418 1015.46 17.3622 Q1013.92 18.9825 1013.92 23.1955 L1013.92 27.2059 L1026.19 27.2059 L1026.19 32.9987 L1013.92 32.9987 L1013.92 72.576 L1006.43 72.576 L1006.43 32.9987 L999.296 32.9987 L999.296 27.2059 L1006.43 27.2059 L1006.43 24.0462 Q1006.43 16.471 1009.95 13.0277 Q1013.47 9.54393 1021.13 9.54393 L1028.18 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1052 32.4315 Q1046 32.4315 1042.52 37.1306 Q1039.04 41.7891 1039.04 49.9314 Q1039.04 58.0738 1042.48 62.7728 Q1045.96 67.4314 1052 67.4314 Q1057.95 67.4314 1061.44 62.7323 Q1064.92 58.0333 1064.92 49.9314 Q1064.92 41.8701 1061.44 37.1711 Q1057.95 32.4315 1052 32.4315 M1052 26.1121 Q1061.72 26.1121 1067.27 32.4315 Q1072.82 38.7509 1072.82 49.9314 Q1072.82 61.0714 1067.27 67.4314 Q1061.72 73.7508 1052 73.7508 Q1042.24 73.7508 1036.69 67.4314 Q1031.18 61.0714 1031.18 49.9314 Q1031.18 38.7509 1036.69 32.4315 Q1042.24 26.1121 1052 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1102.76 32.4315 Q1096.76 32.4315 1093.28 37.1306 Q1089.79 41.7891 1089.79 49.9314 Q1089.79 58.0738 1093.24 62.7728 Q1096.72 67.4314 1102.76 67.4314 Q1108.71 67.4314 1112.2 62.7323 Q1115.68 58.0333 1115.68 49.9314 Q1115.68 41.8701 1112.2 37.1711 Q1108.71 32.4315 1102.76 32.4315 M1102.76 26.1121 Q1112.48 26.1121 1118.03 32.4315 Q1123.58 38.7509 1123.58 49.9314 Q1123.58 61.0714 1118.03 67.4314 Q1112.48 73.7508 1102.76 73.7508 Q1092.99 73.7508 1087.44 67.4314 Q1081.93 61.0714 1081.93 49.9314 Q1081.93 38.7509 1087.44 32.4315 Q1092.99 26.1121 1102.76 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1165.79 34.0924 L1165.79 9.54393 L1173.24 9.54393 L1173.24 72.576 L1165.79 72.576 L1165.79 65.7705 Q1163.44 69.8214 1159.83 71.8063 Q1156.27 73.7508 1151.25 73.7508 Q1143.02 73.7508 1137.84 67.1883 Q1132.69 60.6258 1132.69 49.9314 Q1132.69 39.2371 1137.84 32.6746 Q1143.02 26.1121 1151.25 26.1121 Q1156.27 26.1121 1159.83 28.0971 Q1163.44 30.0415 1165.79 34.0924 M1140.39 49.9314 Q1140.39 58.1548 1143.75 62.8538 Q1147.15 67.5124 1153.07 67.5124 Q1158.98 67.5124 1162.39 62.8538 Q1165.79 58.1548 1165.79 49.9314 Q1165.79 41.7081 1162.39 37.0496 Q1158.98 32.3505 1153.07 32.3505 Q1147.15 32.3505 1143.75 37.0496 Q1140.39 41.7081 1140.39 49.9314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1222.34 14.324 L1222.34 27.2059 L1237.69 27.2059 L1237.69 32.9987 L1222.34 32.9987 L1222.34 57.6282 Q1222.34 63.1779 1223.84 64.7578 Q1225.38 66.3376 1230.04 66.3376 L1237.69 66.3376 L1237.69 72.576 L1230.04 72.576 Q1221.41 72.576 1218.13 69.3758 Q1214.85 66.1351 1214.85 57.6282 L1214.85 32.9987 L1209.38 32.9987 L1209.38 27.2059 L1214.85 27.2059 L1214.85 14.324 L1222.34 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1273.79 34.1734 Q1272.53 33.4443 1271.03 33.1202 Q1269.57 32.7556 1267.79 32.7556 Q1261.47 32.7556 1258.07 36.8875 Q1254.71 40.9789 1254.71 48.6757 L1254.71 72.576 L1247.21 72.576 L1247.21 27.2059 L1254.71 27.2059 L1254.71 34.2544 Q1257.06 30.1225 1260.82 28.1376 Q1264.59 26.1121 1269.98 26.1121 Q1270.75 26.1121 1271.68 26.2337 Q1272.61 26.3147 1273.75 26.5172 L1273.79 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1302.22 49.7694 Q1293.19 49.7694 1289.71 51.8354 Q1286.22 53.9013 1286.22 58.8839 Q1286.22 62.8538 1288.81 65.2034 Q1291.45 67.5124 1295.94 67.5124 Q1302.14 67.5124 1305.87 63.1374 Q1309.64 58.7219 1309.64 51.4303 L1309.64 49.7694 L1302.22 49.7694 M1317.09 46.6907 L1317.09 72.576 L1309.64 72.576 L1309.64 65.6895 Q1307.08 69.8214 1303.28 71.8063 Q1299.47 73.7508 1293.96 73.7508 Q1286.99 73.7508 1282.86 69.8619 Q1278.77 65.9325 1278.77 59.3701 Q1278.77 51.7138 1283.87 47.825 Q1289.02 43.9361 1299.18 43.9361 L1309.64 43.9361 L1309.64 43.2069 Q1309.64 38.0623 1306.23 35.2672 Q1302.87 32.4315 1296.75 32.4315 Q1292.87 32.4315 1289.18 33.3632 Q1285.49 34.295 1282.09 36.1584 L1282.09 29.2718 Q1286.18 27.692 1290.03 26.9223 Q1293.88 26.1121 1297.52 26.1121 Q1307.37 26.1121 1312.23 31.2163 Q1317.09 36.3204 1317.09 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1332.44 27.2059 L1339.9 27.2059 L1339.9 72.576 L1332.44 72.576 L1332.44 27.2059 M1332.44 9.54393 L1339.9 9.54393 L1339.9 18.9825 L1332.44 18.9825 L1332.44 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1393.21 45.1919 L1393.21 72.576 L1385.75 72.576 L1385.75 45.4349 Q1385.75 38.994 1383.24 35.7938 Q1380.73 32.5936 1375.71 32.5936 Q1369.67 32.5936 1366.19 36.4419 Q1362.7 40.2903 1362.7 46.9338 L1362.7 72.576 L1355.21 72.576 L1355.21 27.2059 L1362.7 27.2059 L1362.7 34.2544 Q1365.38 30.163 1368.98 28.1376 Q1372.63 26.1121 1377.37 26.1121 Q1385.19 26.1121 1389.2 30.9732 Q1393.21 35.7938 1393.21 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1408.07 27.2059 L1415.53 27.2059 L1415.53 72.576 L1408.07 72.576 L1408.07 27.2059 M1408.07 9.54393 L1415.53 9.54393 L1415.53 18.9825 L1408.07 18.9825 L1408.07 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1468.84 45.1919 L1468.84 72.576 L1461.38 72.576 L1461.38 45.4349 Q1461.38 38.994 1458.87 35.7938 Q1456.36 32.5936 1451.34 32.5936 Q1445.3 32.5936 1441.82 36.4419 Q1438.33 40.2903 1438.33 46.9338 L1438.33 72.576 L1430.84 72.576 L1430.84 27.2059 L1438.33 27.2059 L1438.33 34.2544 Q1441.01 30.163 1444.61 28.1376 Q1448.26 26.1121 1453 26.1121 Q1460.82 26.1121 1464.83 30.9732 Q1468.84 35.7938 1468.84 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1513.56 49.3643 Q1513.56 41.2625 1510.2 36.8065 Q1506.87 32.3505 1500.84 32.3505 Q1494.84 32.3505 1491.48 36.8065 Q1488.16 41.2625 1488.16 49.3643 Q1488.16 57.4256 1491.48 61.8816 Q1494.84 66.3376 1500.84 66.3376 Q1506.87 66.3376 1510.2 61.8816 Q1513.56 57.4256 1513.56 49.3643 M1521.01 66.9452 Q1521.01 78.5308 1515.87 84.1616 Q1510.72 89.8329 1500.11 89.8329 Q1496.18 89.8329 1492.7 89.2252 Q1489.21 88.6581 1485.93 87.4428 L1485.93 80.1917 Q1489.21 81.9741 1492.41 82.8248 Q1495.61 83.6755 1498.94 83.6755 Q1506.27 83.6755 1509.91 79.8271 Q1513.56 76.0193 1513.56 68.282 L1513.56 64.5957 Q1511.25 68.6061 1507.64 70.5911 Q1504.04 72.576 1499.02 72.576 Q1490.67 72.576 1485.57 66.2161 Q1480.46 59.8562 1480.46 49.3643 Q1480.46 38.832 1485.57 32.472 Q1490.67 26.1121 1499.02 26.1121 Q1504.04 26.1121 1507.64 28.0971 Q1511.25 30.082 1513.56 34.0924 L1513.56 27.2059 L1521.01 27.2059 L1521.01 66.9452 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1562.74 9.54393 L1570.19 9.54393 L1570.19 72.576 L1562.74 72.576 L1562.74 9.54393 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1603.37 32.4315 Q1597.37 32.4315 1593.89 37.1306 Q1590.4 41.7891 1590.4 49.9314 Q1590.4 58.0738 1593.85 62.7728 Q1597.33 67.4314 1603.37 67.4314 Q1609.32 67.4314 1612.81 62.7323 Q1616.29 58.0333 1616.29 49.9314 Q1616.29 41.8701 1612.81 37.1711 Q1609.32 32.4315 1603.37 32.4315 M1603.37 26.1121 Q1613.09 26.1121 1618.64 32.4315 Q1624.19 38.7509 1624.19 49.9314 Q1624.19 61.0714 1618.64 67.4314 Q1613.09 73.7508 1603.37 73.7508 Q1593.6 73.7508 1588.05 67.4314 Q1582.55 61.0714 1582.55 49.9314 Q1582.55 38.7509 1588.05 32.4315 Q1593.6 26.1121 1603.37 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1665.47 28.5427 L1665.47 35.5912 Q1662.31 33.9709 1658.91 33.1607 Q1655.5 32.3505 1651.86 32.3505 Q1646.31 32.3505 1643.51 34.0519 Q1640.76 35.7533 1640.76 39.156 Q1640.76 41.7486 1642.74 43.2475 Q1644.73 44.7058 1650.72 46.0426 L1653.27 46.6097 Q1661.21 48.3111 1664.54 51.4303 Q1667.9 54.509 1667.9 60.0587 Q1667.9 66.3781 1662.88 70.0644 Q1657.89 73.7508 1649.14 73.7508 Q1645.5 73.7508 1641.53 73.0216 Q1637.6 72.3329 1633.22 70.9151 L1633.22 63.2184 Q1637.35 65.3654 1641.36 66.4591 Q1645.38 67.5124 1649.3 67.5124 Q1654.57 67.5124 1657.41 65.73 Q1660.24 63.9071 1660.24 60.6258 Q1660.24 57.5877 1658.18 55.9673 Q1656.15 54.3469 1649.22 52.8481 L1646.63 52.2405 Q1639.7 50.7821 1636.63 47.7845 Q1633.55 44.7463 1633.55 39.4801 Q1633.55 33.0797 1638.08 29.5959 Q1642.62 26.1121 1650.97 26.1121 Q1655.1 26.1121 1658.74 26.7198 Q1662.39 27.3274 1665.47 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip070)\" d=\"M1708.69 28.5427 L1708.69 35.5912 Q1705.53 33.9709 1702.13 33.1607 Q1698.73 32.3505 1695.08 32.3505 Q1689.53 32.3505 1686.73 34.0519 Q1683.98 35.7533 1683.98 39.156 Q1683.98 41.7486 1685.97 43.2475 Q1687.95 44.7058 1693.95 46.0426 L1696.5 46.6097 Q1704.44 48.3111 1707.76 51.4303 Q1711.12 54.509 1711.12 60.0587 Q1711.12 66.3781 1706.1 70.0644 Q1701.12 73.7508 1692.37 73.7508 Q1688.72 73.7508 1684.75 73.0216 Q1680.82 72.3329 1676.45 70.9151 L1676.45 63.2184 Q1680.58 65.3654 1684.59 66.4591 Q1688.6 67.5124 1692.53 67.5124 Q1697.79 67.5124 1700.63 65.73 Q1703.47 63.9071 1703.47 60.6258 Q1703.47 57.5877 1701.4 55.9673 Q1699.37 54.3469 1692.45 52.8481 L1689.85 52.2405 Q1682.93 50.7821 1679.85 47.7845 Q1676.77 44.7463 1676.77 39.4801 Q1676.77 33.0797 1681.31 29.5959 Q1685.84 26.1121 1694.19 26.1121 Q1698.32 26.1121 1701.97 26.7198 Q1705.61 27.3274 1708.69 28.5427 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip072)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  217.944,162.047 218.981,224.612 220.018,286.447 221.054,348.255 222.091,408.963 223.128,469.74 224.165,534.262 225.202,597.114 226.239,654.672 227.275,711.508 \n",
       "  228.312,763.349 229.349,810.447 230.386,855.368 231.423,894.176 232.46,930.184 233.496,963.526 234.533,996.75 235.57,1029.13 236.607,1059.52 237.644,1088.29 \n",
       "  238.681,1115.9 239.717,1142.21 240.754,1167.06 241.791,1189.57 242.828,1210.48 243.865,1229.9 244.902,1247.61 245.938,1263.92 246.975,1278.89 248.012,1291.91 \n",
       "  249.049,1303.26 250.086,1314.28 251.123,1325.07 252.159,1335.44 253.196,1344.02 254.233,1351.71 255.27,1358.75 256.307,1365.2 257.344,1371.1 258.38,1376.49 \n",
       "  259.417,1381.42 260.454,1385.92 261.491,1390.03 262.528,1393.79 263.565,1397.22 264.601,1400.36 265.638,1403.24 266.675,1405.87 267.712,1408.28 268.749,1410.49 \n",
       "  269.786,1412.53 270.822,1414.4 271.859,1416.12 272.896,1417.7 273.933,1419.17 274.97,1420.52 276.007,1421.78 277.043,1422.94 278.08,1424.02 279.117,1425.02 \n",
       "  280.154,1425.95 281.191,1426.82 282.228,1427.49 283.264,1428.16 284.301,1428.97 285.338,1429.57 286.375,1430.12 287.412,1430.64 288.449,1431.14 289.486,1431.62 \n",
       "  290.522,1432.07 291.559,1432.51 292.596,1432.92 293.633,1433.32 294.67,1433.71 295.707,1434.07 296.743,1434.42 297.78,1434.76 298.817,1435.09 299.854,1435.4 \n",
       "  300.891,1435.7 301.928,1435.99 302.964,1436.26 304.001,1436.53 305.038,1436.79 306.075,1437.04 307.112,1437.28 308.149,1437.51 309.185,1437.73 310.222,1437.95 \n",
       "  311.259,1438.16 312.296,1438.35 313.333,1438.53 314.37,1438.7 315.406,1438.87 316.443,1439.04 317.48,1439.2 318.517,1439.36 319.554,1439.52 320.591,1439.67 \n",
       "  321.627,1439.82 322.664,1439.96 323.701,1440.1 324.738,1440.22 325.775,1440.35 326.812,1440.47 327.848,1440.58 328.885,1440.69 329.922,1440.8 330.959,1440.91 \n",
       "  331.996,1441.02 333.033,1441.12 334.069,1441.23 335.106,1441.33 336.143,1441.43 337.18,1441.53 338.217,1441.63 339.254,1441.73 340.29,1441.82 341.327,1441.92 \n",
       "  342.364,1442.01 343.401,1442.1 344.438,1442.19 345.475,1442.27 346.511,1442.36 347.548,1442.36 348.585,1442.5 349.622,1442.58 350.659,1442.65 351.696,1442.72 \n",
       "  352.732,1442.78 353.769,1442.85 354.806,1442.91 355.843,1442.98 356.88,1443.04 357.917,1443.11 358.953,1443.17 359.99,1443.23 361.027,1443.3 362.064,1443.36 \n",
       "  363.101,1443.42 364.138,1443.48 365.174,1443.54 366.211,1443.6 367.248,1443.66 368.285,1443.71 369.322,1443.71 370.359,1443.81 371.395,1443.85 372.432,1443.89 \n",
       "  373.469,1443.93 374.506,1443.97 375.543,1444.02 376.58,1444.06 377.616,1444.1 378.653,1444.15 379.69,1444.19 380.727,1444.23 381.764,1444.27 382.801,1444.32 \n",
       "  383.837,1444.36 384.874,1444.4 385.911,1444.43 386.948,1444.46 387.985,1444.49 389.022,1444.52 390.058,1444.55 391.095,1444.58 392.132,1444.62 393.169,1444.65 \n",
       "  394.206,1444.68 395.243,1444.72 396.28,1444.75 397.316,1444.79 398.353,1444.82 399.39,1444.86 400.427,1444.89 401.464,1444.93 402.501,1444.96 403.537,1444.99 \n",
       "  404.574,1445.03 405.611,1445.06 406.648,1445.09 407.685,1445.13 408.722,1445.16 409.758,1445.19 410.795,1445.22 411.832,1445.26 412.869,1445.29 413.906,1445.32 \n",
       "  414.943,1445.35 415.979,1445.38 417.016,1445.41 418.053,1445.44 419.09,1445.47 420.127,1445.5 421.164,1445.53 422.2,1445.56 423.237,1445.58 424.274,1445.61 \n",
       "  425.311,1445.64 426.348,1445.66 427.385,1445.69 428.421,1445.72 429.458,1445.74 430.495,1445.77 431.532,1445.79 432.569,1445.82 433.606,1445.84 434.642,1445.87 \n",
       "  435.679,1445.89 436.716,1445.87 437.753,1445.93 438.79,1445.95 439.827,1445.97 440.863,1445.99 441.9,1446 442.937,1446.02 443.974,1446.04 445.011,1446.06 \n",
       "  446.048,1446.08 447.084,1446.1 448.121,1446.12 449.158,1446.14 450.195,1446.15 451.232,1446.17 452.269,1446.19 453.305,1446.21 454.342,1446.23 455.379,1446.25 \n",
       "  456.416,1446.26 457.453,1446.28 458.49,1446.3 459.526,1446.32 460.563,1446.34 461.6,1446.35 462.637,1446.36 463.674,1446.38 464.711,1446.38 465.747,1446.39 \n",
       "  466.784,1446.4 467.821,1446.41 468.858,1446.42 469.895,1446.43 470.932,1446.44 471.968,1446.45 473.005,1446.46 474.042,1446.47 475.079,1446.48 476.116,1446.5 \n",
       "  477.153,1446.51 478.189,1446.51 479.226,1446.51 480.263,1446.51 481.3,1446.51 482.337,1446.52 483.374,1446.52 484.41,1446.52 485.447,1446.52 486.484,1446.52 \n",
       "  487.521,1446.52 488.558,1446.52 489.595,1446.53 490.631,1446.53 491.668,1446.53 492.705,1446.53 493.742,1446.53 494.779,1446.53 495.816,1446.53 496.852,1446.54 \n",
       "  497.889,1446.54 498.926,1446.54 499.963,1446.54 501,1446.54 502.037,1446.54 503.074,1446.55 504.11,1446.55 505.147,1446.55 506.184,1446.55 507.221,1446.55 \n",
       "  508.258,1446.55 509.295,1446.55 510.331,1446.56 511.368,1446.56 512.405,1446.56 513.442,1446.56 514.479,1446.56 515.516,1446.56 516.552,1446.56 517.589,1446.57 \n",
       "  518.626,1446.57 519.663,1446.57 520.7,1446.57 521.737,1446.57 522.773,1446.57 523.81,1446.58 524.847,1446.58 525.884,1446.58 526.921,1446.58 527.958,1446.58 \n",
       "  528.994,1446.58 530.031,1446.59 531.068,1446.59 532.105,1446.59 533.142,1446.59 534.179,1446.59 535.215,1446.59 536.252,1446.59 537.289,1446.6 538.326,1446.6 \n",
       "  539.363,1446.6 540.4,1446.6 541.436,1446.6 542.473,1446.6 543.51,1446.61 544.547,1446.61 545.584,1446.61 546.621,1446.61 547.657,1446.61 548.694,1446.61 \n",
       "  549.731,1446.62 550.768,1446.62 551.805,1446.62 552.842,1446.62 553.878,1446.62 554.915,1446.62 555.952,1446.62 556.989,1446.63 558.026,1446.63 559.063,1446.63 \n",
       "  560.099,1446.63 561.136,1446.63 562.173,1446.63 563.21,1446.64 564.247,1446.64 565.284,1446.64 566.32,1446.64 567.357,1446.64 568.394,1446.64 569.431,1446.65 \n",
       "  570.468,1446.65 571.505,1446.65 572.541,1446.65 573.578,1446.65 574.615,1446.65 575.652,1446.65 576.689,1446.66 577.726,1446.66 578.762,1446.66 579.799,1446.66 \n",
       "  580.836,1446.66 581.873,1446.66 582.91,1446.67 583.947,1446.67 584.983,1446.67 586.02,1446.67 587.057,1446.67 588.094,1446.67 589.131,1446.68 590.168,1446.68 \n",
       "  591.204,1446.68 592.241,1446.68 593.278,1446.68 594.315,1446.68 595.352,1446.68 596.389,1446.69 597.425,1446.69 598.462,1446.69 599.499,1446.69 600.536,1446.69 \n",
       "  601.573,1446.69 602.61,1446.69 603.646,1446.7 604.683,1446.7 605.72,1446.7 606.757,1446.7 607.794,1446.7 608.831,1446.7 609.867,1446.7 610.904,1446.7 \n",
       "  611.941,1446.7 612.978,1446.7 614.015,1446.7 615.052,1446.71 616.089,1446.71 617.125,1446.71 618.162,1446.71 619.199,1446.71 620.236,1446.71 621.273,1446.71 \n",
       "  622.31,1446.72 623.346,1446.72 624.383,1446.72 625.42,1446.72 626.457,1446.72 627.494,1446.72 628.531,1446.72 629.567,1446.73 630.604,1446.73 631.641,1446.73 \n",
       "  632.678,1446.73 633.715,1446.73 634.752,1446.73 635.788,1446.74 636.825,1446.74 637.862,1446.74 638.899,1446.74 639.936,1446.74 640.973,1446.74 642.009,1446.74 \n",
       "  643.046,1446.75 644.083,1446.75 645.12,1446.75 646.157,1446.75 647.194,1446.75 648.23,1446.75 649.267,1446.76 650.304,1446.76 651.341,1446.76 652.378,1446.76 \n",
       "  653.415,1446.76 654.451,1446.76 655.488,1446.77 656.525,1446.77 657.562,1446.77 658.599,1446.77 659.636,1446.77 660.672,1446.77 661.709,1446.78 662.746,1446.78 \n",
       "  663.783,1446.78 664.82,1446.78 665.857,1446.78 666.893,1446.78 667.93,1446.78 668.967,1446.79 670.004,1446.79 671.041,1446.79 672.078,1446.79 673.114,1446.79 \n",
       "  674.151,1446.79 675.188,1446.8 676.225,1446.8 677.262,1446.8 678.299,1446.8 679.335,1446.8 680.372,1446.8 681.409,1446.81 682.446,1446.81 683.483,1446.81 \n",
       "  684.52,1446.81 685.556,1446.81 686.593,1446.81 687.63,1446.82 688.667,1446.82 689.704,1446.82 690.741,1446.82 691.777,1446.82 692.814,1446.82 693.851,1446.83 \n",
       "  694.888,1446.83 695.925,1446.83 696.962,1446.83 697.998,1446.83 699.035,1446.83 700.072,1446.83 701.109,1446.84 702.146,1446.84 703.183,1446.84 704.219,1446.84 \n",
       "  705.256,1446.84 706.293,1446.84 707.33,1446.85 708.367,1446.85 709.404,1446.85 710.44,1446.85 711.477,1446.85 712.514,1446.85 713.551,1446.85 714.588,1446.85 \n",
       "  715.625,1446.85 716.661,1446.86 717.698,1446.86 718.735,1446.86 719.772,1446.86 720.809,1446.86 721.846,1446.86 722.883,1446.86 723.919,1446.86 724.956,1446.86 \n",
       "  725.993,1446.86 727.03,1446.87 728.067,1446.87 729.104,1446.87 730.14,1446.87 731.177,1446.87 732.214,1446.87 733.251,1446.87 734.288,1446.88 735.325,1446.88 \n",
       "  736.361,1446.88 737.398,1446.88 738.435,1446.88 739.472,1446.88 740.509,1446.88 741.546,1446.88 742.582,1446.88 743.619,1446.88 744.656,1446.88 745.693,1446.88 \n",
       "  746.73,1446.89 747.767,1446.89 748.803,1446.89 749.84,1446.89 750.877,1446.89 751.914,1446.89 752.951,1446.89 753.988,1446.89 755.024,1446.89 756.061,1446.89 \n",
       "  757.098,1446.89 758.135,1446.89 759.172,1446.89 760.209,1446.9 761.245,1446.9 762.282,1446.9 763.319,1446.9 764.356,1446.9 765.393,1446.9 766.43,1446.9 \n",
       "  767.466,1446.9 768.503,1446.9 769.54,1446.9 770.577,1446.9 771.614,1446.9 772.651,1446.91 773.687,1446.91 774.724,1446.91 775.761,1446.91 776.798,1446.91 \n",
       "  777.835,1446.91 778.872,1446.91 779.908,1446.91 780.945,1446.91 781.982,1446.91 783.019,1446.91 784.056,1446.91 785.093,1446.92 786.129,1446.92 787.166,1446.92 \n",
       "  788.203,1446.92 789.24,1446.92 790.277,1446.92 791.314,1446.92 792.35,1446.92 793.387,1446.92 794.424,1446.92 795.461,1446.92 796.498,1446.92 797.535,1446.92 \n",
       "  798.571,1446.93 799.608,1446.93 800.645,1446.93 801.682,1446.93 802.719,1446.93 803.756,1446.93 804.792,1446.93 805.829,1446.93 806.866,1446.93 807.903,1446.93 \n",
       "  808.94,1446.93 809.977,1446.93 811.013,1446.94 812.05,1446.94 813.087,1446.94 814.124,1446.94 815.161,1446.94 816.198,1446.94 817.234,1446.94 818.271,1446.94 \n",
       "  819.308,1446.94 820.345,1446.94 821.382,1446.94 822.419,1446.94 823.455,1446.95 824.492,1446.95 825.529,1446.95 826.566,1446.95 827.603,1446.95 828.64,1446.95 \n",
       "  829.677,1446.95 830.713,1446.95 831.75,1446.95 832.787,1446.95 833.824,1446.95 834.861,1446.95 835.898,1446.96 836.934,1446.96 837.971,1446.96 839.008,1446.96 \n",
       "  840.045,1446.96 841.082,1446.96 842.119,1446.96 843.155,1446.96 844.192,1446.96 845.229,1446.96 846.266,1446.96 847.303,1446.96 848.34,1446.96 849.376,1446.97 \n",
       "  850.413,1446.97 851.45,1446.97 852.487,1446.97 853.524,1446.97 854.561,1446.97 855.597,1446.97 856.634,1446.97 857.671,1446.97 858.708,1446.97 859.745,1446.97 \n",
       "  860.782,1446.97 861.818,1446.98 862.855,1446.98 863.892,1446.98 864.929,1446.98 865.966,1446.98 867.003,1446.98 868.039,1446.98 869.076,1446.98 870.113,1446.98 \n",
       "  871.15,1446.98 872.187,1446.98 873.224,1446.98 874.26,1446.99 875.297,1446.99 876.334,1446.99 877.371,1446.99 878.408,1446.99 879.445,1446.99 880.481,1446.99 \n",
       "  881.518,1446.99 882.555,1446.99 883.592,1446.99 884.629,1446.99 885.666,1446.99 886.702,1447 887.739,1447 888.776,1447 889.813,1447 890.85,1447 \n",
       "  891.887,1447 892.923,1447 893.96,1447 894.997,1447 896.034,1447 897.071,1447 898.108,1447 899.144,1447.01 900.181,1447.01 901.218,1447.01 \n",
       "  902.255,1447.01 903.292,1447.01 904.329,1447.01 905.365,1447.01 906.402,1447.01 907.439,1447.01 908.476,1447.01 909.513,1447.01 910.55,1447.01 911.586,1447.02 \n",
       "  912.623,1447.02 913.66,1447.02 914.697,1447.02 915.734,1447.02 916.771,1447.02 917.807,1447.02 918.844,1447.02 919.881,1447.02 920.918,1447.02 921.955,1447.02 \n",
       "  922.992,1447.02 924.028,1447.02 925.065,1447.02 926.102,1447.02 927.139,1447.02 928.176,1447.02 929.213,1447.02 930.249,1447.02 931.286,1447.02 932.323,1447.03 \n",
       "  933.36,1447.03 934.397,1447.03 935.434,1447.03 936.471,1447.03 937.507,1447.03 938.544,1447.03 939.581,1447.03 940.618,1447.03 941.655,1447.03 942.692,1447.03 \n",
       "  943.728,1447.03 944.765,1447.03 945.802,1447.04 946.839,1447.04 947.876,1447.04 948.913,1447.04 949.949,1447.04 950.986,1447.04 952.023,1447.04 953.06,1447.04 \n",
       "  954.097,1447.04 955.134,1447.04 956.17,1447.04 957.207,1447.04 958.244,1447.04 959.281,1447.05 960.318,1447.05 961.355,1447.05 962.391,1447.05 963.428,1447.05 \n",
       "  964.465,1447.05 965.502,1447.05 966.539,1447.05 967.576,1447.05 968.612,1447.05 969.649,1447.05 970.686,1447.05 971.723,1447.06 972.76,1447.06 973.797,1447.06 \n",
       "  974.833,1447.06 975.87,1447.06 976.907,1447.06 977.944,1447.06 978.981,1447.06 980.018,1447.06 981.054,1447.06 982.091,1447.06 983.128,1447.06 984.165,1447.07 \n",
       "  985.202,1447.07 986.239,1447.07 987.275,1447.07 988.312,1447.07 989.349,1447.07 990.386,1447.07 991.423,1447.07 992.46,1447.07 993.496,1447.07 994.533,1447.07 \n",
       "  995.57,1447.07 996.607,1447.08 997.644,1447.08 998.681,1447.08 999.717,1447.08 1000.75,1447.08 1001.79,1447.08 1002.83,1447.08 1003.86,1447.08 1004.9,1447.08 \n",
       "  1005.94,1447.08 1006.98,1447.08 1008.01,1447.08 1009.05,1447.09 1010.09,1447.09 1011.12,1447.09 1012.16,1447.09 1013.2,1447.09 1014.23,1447.09 1015.27,1447.09 \n",
       "  1016.31,1447.09 1017.34,1447.09 1018.38,1447.09 1019.42,1447.09 1020.45,1447.09 1021.49,1447.1 1022.53,1447.1 1023.56,1447.1 1024.6,1447.1 1025.64,1447.1 \n",
       "  1026.68,1447.1 1027.71,1447.1 1028.75,1447.1 1029.79,1447.1 1030.82,1447.1 1031.86,1447.1 1032.9,1447.1 1033.93,1447.11 1034.97,1447.11 1036.01,1447.11 \n",
       "  1037.04,1447.11 1038.08,1447.11 1039.12,1447.11 1040.15,1447.11 1041.19,1447.11 1042.23,1447.11 1043.26,1447.11 1044.3,1447.11 1045.34,1447.11 1046.38,1447.11 \n",
       "  1047.41,1447.11 1048.45,1447.12 1049.49,1447.12 1050.52,1447.12 1051.56,1447.12 1052.6,1447.12 1053.63,1447.12 1054.67,1447.12 1055.71,1447.12 1056.74,1447.12 \n",
       "  1057.78,1447.12 1058.82,1447.12 1059.85,1447.12 1060.89,1447.12 1061.93,1447.13 1062.96,1447.13 1064,1447.13 1065.04,1447.13 1066.07,1447.13 1067.11,1447.13 \n",
       "  1068.15,1447.13 1069.19,1447.13 1070.22,1447.13 1071.26,1447.13 1072.3,1447.13 1073.33,1447.13 1074.37,1447.13 1075.41,1447.14 1076.44,1447.14 1077.48,1447.14 \n",
       "  1078.52,1447.14 1079.55,1447.14 1080.59,1447.14 1081.63,1447.14 1082.66,1447.14 1083.7,1447.14 1084.74,1447.14 1085.77,1447.14 1086.81,1447.14 1087.85,1447.15 \n",
       "  1088.89,1447.15 1089.92,1447.15 1090.96,1447.15 1092,1447.15 1093.03,1447.15 1094.07,1447.15 1095.11,1447.15 1096.14,1447.15 1097.18,1447.15 1098.22,1447.15 \n",
       "  1099.25,1447.15 1100.29,1447.15 1101.33,1447.15 1102.36,1447.15 1103.4,1447.15 1104.44,1447.15 1105.47,1447.15 1106.51,1447.15 1107.55,1447.15 1108.59,1447.16 \n",
       "  1109.62,1447.16 1110.66,1447.16 1111.7,1447.16 1112.73,1447.16 1113.77,1447.16 1114.81,1447.16 1115.84,1447.16 1116.88,1447.16 1117.92,1447.16 1118.95,1447.16 \n",
       "  1119.99,1447.16 1121.03,1447.16 1122.06,1447.16 1123.1,1447.17 1124.14,1447.17 1125.17,1447.17 1126.21,1447.17 1127.25,1447.17 1128.28,1447.17 1129.32,1447.17 \n",
       "  1130.36,1447.17 1131.4,1447.17 1132.43,1447.17 1133.47,1447.17 1134.51,1447.17 1135.54,1447.18 1136.58,1447.18 1137.62,1447.18 1138.65,1447.18 1139.69,1447.18 \n",
       "  1140.73,1447.18 1141.76,1447.18 1142.8,1447.18 1143.84,1447.18 1144.87,1447.18 1145.91,1447.18 1146.95,1447.18 1147.98,1447.19 1149.02,1447.19 1150.06,1447.19 \n",
       "  1151.1,1447.19 1152.13,1447.19 1153.17,1447.19 1154.21,1447.19 1155.24,1447.19 1156.28,1447.19 1157.32,1447.19 1158.35,1447.19 1159.39,1447.19 1160.43,1447.2 \n",
       "  1161.46,1447.2 1162.5,1447.2 1163.54,1447.2 1164.57,1447.2 1165.61,1447.2 1166.65,1447.2 1167.68,1447.2 1168.72,1447.2 1169.76,1447.2 1170.8,1447.2 \n",
       "  1171.83,1447.2 1172.87,1447.21 1173.91,1447.21 1174.94,1447.21 1175.98,1447.21 1177.02,1447.21 1178.05,1447.21 1179.09,1447.21 1180.13,1447.21 1181.16,1447.21 \n",
       "  1182.2,1447.21 1183.24,1447.21 1184.27,1447.21 1185.31,1447.22 1186.35,1447.22 1187.38,1447.22 1188.42,1447.22 1189.46,1447.22 1190.5,1447.22 1191.53,1447.22 \n",
       "  1192.57,1447.22 1193.61,1447.22 1194.64,1447.22 1195.68,1447.22 1196.72,1447.22 1197.75,1447.22 1198.79,1447.23 1199.83,1447.23 1200.86,1447.23 1201.9,1447.23 \n",
       "  1202.94,1447.23 1203.97,1447.23 1205.01,1447.23 1206.05,1447.23 1207.08,1447.23 1208.12,1447.23 1209.16,1447.23 1210.19,1447.23 1211.23,1447.23 1212.27,1447.24 \n",
       "  1213.31,1447.24 1214.34,1447.24 1215.38,1447.24 1216.42,1447.24 1217.45,1447.24 1218.49,1447.24 1219.53,1447.24 1220.56,1447.24 1221.6,1447.24 1222.64,1447.24 \n",
       "  1223.67,1447.24 1224.71,1447.24 1225.75,1447.24 1226.78,1447.24 1227.82,1447.25 1228.86,1447.25 1229.89,1447.25 1230.93,1447.25 1231.97,1447.25 1233.01,1447.25 \n",
       "  1234.04,1447.25 1235.08,1447.25 1236.12,1447.25 1237.15,1447.25 1238.19,1447.25 1239.23,1447.25 1240.26,1447.25 1241.3,1447.26 1242.34,1447.26 1243.37,1447.26 \n",
       "  1244.41,1447.26 1245.45,1447.26 1246.48,1447.26 1247.52,1447.26 1248.56,1447.26 1249.59,1447.26 1250.63,1447.26 1251.67,1447.26 1252.71,1447.26 1253.74,1447.26 \n",
       "  1254.78,1447.27 1255.82,1447.27 1256.85,1447.27 1257.89,1447.27 1258.93,1447.27 1259.96,1447.27 1261,1447.27 1262.04,1447.27 1263.07,1447.27 1264.11,1447.27 \n",
       "  1265.15,1447.27 1266.18,1447.27 1267.22,1447.28 1268.26,1447.28 1269.29,1447.28 1270.33,1447.28 1271.37,1447.28 1272.41,1447.28 1273.44,1447.28 1274.48,1447.28 \n",
       "  1275.52,1447.28 1276.55,1447.28 1277.59,1447.28 1278.63,1447.28 1279.66,1447.29 1280.7,1447.29 1281.74,1447.29 1282.77,1447.29 1283.81,1447.29 1284.85,1447.29 \n",
       "  1285.88,1447.29 1286.92,1447.29 1287.96,1447.29 1288.99,1447.29 1290.03,1447.29 1291.07,1447.29 1292.1,1447.29 1293.14,1447.29 1294.18,1447.29 1295.22,1447.29 \n",
       "  1296.25,1447.29 1297.29,1447.3 1298.33,1447.3 1299.36,1447.3 1300.4,1447.3 1301.44,1447.3 1302.47,1447.3 1303.51,1447.3 1304.55,1447.3 1305.58,1447.3 \n",
       "  1306.62,1447.3 1307.66,1447.3 1308.69,1447.3 1309.73,1447.3 1310.77,1447.31 1311.8,1447.31 1312.84,1447.31 1313.88,1447.31 1314.92,1447.31 1315.95,1447.31 \n",
       "  1316.99,1447.31 1318.03,1447.31 1319.06,1447.31 1320.1,1447.31 1321.14,1447.31 1322.17,1447.31 1323.21,1447.31 1324.25,1447.32 1325.28,1447.32 1326.32,1447.32 \n",
       "  1327.36,1447.32 1328.39,1447.32 1329.43,1447.32 1330.47,1447.32 1331.5,1447.32 1332.54,1447.32 1333.58,1447.32 1334.62,1447.32 1335.65,1447.32 1336.69,1447.33 \n",
       "  1337.73,1447.33 1338.76,1447.33 1339.8,1447.33 1340.84,1447.33 1341.87,1447.33 1342.91,1447.33 1343.95,1447.33 1344.98,1447.33 1346.02,1447.33 1347.06,1447.33 \n",
       "  1348.09,1447.33 1349.13,1447.33 1350.17,1447.33 1351.2,1447.33 1352.24,1447.33 1353.28,1447.33 1354.31,1447.33 1355.35,1447.33 1356.39,1447.33 1357.43,1447.33 \n",
       "  1358.46,1447.33 1359.5,1447.33 1360.54,1447.34 1361.57,1447.34 1362.61,1447.34 1363.65,1447.34 1364.68,1447.34 1365.72,1447.34 1366.76,1447.34 1367.79,1447.34 \n",
       "  1368.83,1447.34 1369.87,1447.34 1370.9,1447.34 1371.94,1447.34 1372.98,1447.34 1374.01,1447.34 1375.05,1447.35 1376.09,1447.35 1377.13,1447.35 1378.16,1447.35 \n",
       "  1379.2,1447.35 1380.24,1447.35 1381.27,1447.35 1382.31,1447.35 1383.35,1447.35 1384.38,1447.35 1385.42,1447.35 1386.46,1447.35 1387.49,1447.35 1388.53,1447.36 \n",
       "  1389.57,1447.36 1390.6,1447.36 1391.64,1447.36 1392.68,1447.36 1393.71,1447.36 1394.75,1447.36 1395.79,1447.36 1396.83,1447.36 1397.86,1447.36 1398.9,1447.36 \n",
       "  1399.94,1447.36 1400.97,1447.37 1402.01,1447.37 1403.05,1447.37 1404.08,1447.37 1405.12,1447.37 1406.16,1447.37 1407.19,1447.37 1408.23,1447.37 1409.27,1447.37 \n",
       "  1410.3,1447.37 1411.34,1447.37 1412.38,1447.37 1413.41,1447.37 1414.45,1447.38 1415.49,1447.38 1416.53,1447.38 1417.56,1447.38 1418.6,1447.38 1419.64,1447.38 \n",
       "  1420.67,1447.38 1421.71,1447.38 1422.75,1447.38 1423.78,1447.38 1424.82,1447.38 1425.86,1447.38 1426.89,1447.38 1427.93,1447.39 1428.97,1447.39 1430,1447.39 \n",
       "  1431.04,1447.39 1432.08,1447.39 1433.11,1447.39 1434.15,1447.39 1435.19,1447.39 1436.22,1447.39 1437.26,1447.39 1438.3,1447.39 1439.34,1447.39 1440.37,1447.39 \n",
       "  1441.41,1447.4 1442.45,1447.4 1443.48,1447.4 1444.52,1447.4 1445.56,1447.4 1446.59,1447.4 1447.63,1447.4 1448.67,1447.4 1449.7,1447.4 1450.74,1447.4 \n",
       "  1451.78,1447.4 1452.81,1447.4 1453.85,1447.41 1454.89,1447.41 1455.92,1447.41 1456.96,1447.41 1458,1447.41 1459.04,1447.41 1460.07,1447.41 1461.11,1447.41 \n",
       "  1462.15,1447.41 1463.18,1447.41 1464.22,1447.41 1465.26,1447.41 1466.29,1447.41 1467.33,1447.41 1468.37,1447.42 1469.4,1447.42 1470.44,1447.42 1471.48,1447.42 \n",
       "  1472.51,1447.42 1473.55,1447.42 1474.59,1447.42 1475.62,1447.42 1476.66,1447.42 1477.7,1447.42 1478.74,1447.42 1479.77,1447.42 1480.81,1447.42 1481.85,1447.42 \n",
       "  1482.88,1447.42 1483.92,1447.42 1484.96,1447.42 1485.99,1447.43 1487.03,1447.43 1488.07,1447.43 1489.1,1447.43 1490.14,1447.43 1491.18,1447.43 1492.21,1447.43 \n",
       "  1493.25,1447.43 1494.29,1447.43 1495.32,1447.43 1496.36,1447.43 1497.4,1447.43 1498.44,1447.43 1499.47,1447.43 1500.51,1447.44 1501.55,1447.44 1502.58,1447.44 \n",
       "  1503.62,1447.44 1504.66,1447.44 1505.69,1447.44 1506.73,1447.44 1507.77,1447.44 1508.8,1447.44 1509.84,1447.44 1510.88,1447.44 1511.91,1447.44 1512.95,1447.44 \n",
       "  1513.99,1447.45 1515.02,1447.45 1516.06,1447.45 1517.1,1447.45 1518.13,1447.45 1519.17,1447.45 1520.21,1447.45 1521.25,1447.45 1522.28,1447.45 1523.32,1447.45 \n",
       "  1524.36,1447.45 1525.39,1447.45 1526.43,1447.45 1527.47,1447.45 1528.5,1447.46 1529.54,1447.46 1530.58,1447.46 1531.61,1447.46 1532.65,1447.46 1533.69,1447.46 \n",
       "  1534.72,1447.46 1535.76,1447.46 1536.8,1447.46 1537.83,1447.46 1538.87,1447.46 1539.91,1447.46 1540.95,1447.46 1541.98,1447.47 1543.02,1447.47 1544.06,1447.47 \n",
       "  1545.09,1447.47 1546.13,1447.47 1547.17,1447.47 1548.2,1447.47 1549.24,1447.47 1550.28,1447.47 1551.31,1447.47 1552.35,1447.47 1553.39,1447.47 1554.42,1447.47 \n",
       "  1555.46,1447.47 1556.5,1447.47 1557.53,1447.47 1558.57,1447.47 1559.61,1447.48 1560.65,1447.48 1561.68,1447.48 1562.72,1447.48 1563.76,1447.48 1564.79,1447.48 \n",
       "  1565.83,1447.48 1566.87,1447.48 1567.9,1447.48 1568.94,1447.48 1569.98,1447.48 1571.01,1447.48 1572.05,1447.48 1573.09,1447.48 1574.12,1447.48 1575.16,1447.48 \n",
       "  1576.2,1447.49 1577.23,1447.49 1578.27,1447.49 1579.31,1447.49 1580.34,1447.49 1581.38,1447.49 1582.42,1447.49 1583.46,1447.49 1584.49,1447.49 1585.53,1447.49 \n",
       "  1586.57,1447.49 1587.6,1447.49 1588.64,1447.49 1589.68,1447.49 1590.71,1447.5 1591.75,1447.5 1592.79,1447.5 1593.82,1447.5 1594.86,1447.5 1595.9,1447.5 \n",
       "  1596.93,1447.5 1597.97,1447.5 1599.01,1447.5 1600.04,1447.5 1601.08,1447.5 1602.12,1447.5 1603.16,1447.5 1604.19,1447.51 1605.23,1447.51 1606.27,1447.51 \n",
       "  1607.3,1447.51 1608.34,1447.51 1609.38,1447.51 1610.41,1447.51 1611.45,1447.51 1612.49,1447.51 1613.52,1447.51 1614.56,1447.51 1615.6,1447.51 1616.63,1447.51 \n",
       "  1617.67,1447.51 1618.71,1447.52 1619.74,1447.52 1620.78,1447.52 1621.82,1447.52 1622.86,1447.51 1623.89,1447.52 1624.93,1447.52 1625.97,1447.52 1627,1447.52 \n",
       "  1628.04,1447.52 1629.08,1447.52 1630.11,1447.52 1631.15,1447.52 1632.19,1447.52 1633.22,1447.52 1634.26,1447.52 1635.3,1447.52 1636.33,1447.52 1637.37,1447.52 \n",
       "  1638.41,1447.52 1639.44,1447.52 1640.48,1447.52 1641.52,1447.52 1642.56,1447.52 1643.59,1447.52 1644.63,1447.52 1645.67,1447.52 1646.7,1447.53 1647.74,1447.53 \n",
       "  1648.78,1447.53 1649.81,1447.53 1650.85,1447.53 1651.89,1447.53 1652.92,1447.53 1653.96,1447.53 1655,1447.53 1656.03,1447.53 1657.07,1447.53 1658.11,1447.53 \n",
       "  1659.14,1447.53 1660.18,1447.53 1661.22,1447.54 1662.25,1447.54 1663.29,1447.54 1664.33,1447.54 1665.37,1447.54 1666.4,1447.54 1667.44,1447.54 1668.48,1447.54 \n",
       "  1669.51,1447.54 1670.55,1447.54 1671.59,1447.54 1672.62,1447.54 1673.66,1447.54 1674.7,1447.54 1675.73,1447.54 1676.77,1447.55 1677.81,1447.55 1678.84,1447.55 \n",
       "  1679.88,1447.55 1680.92,1447.55 1681.95,1447.55 1682.99,1447.55 1684.03,1447.55 1685.07,1447.55 1686.1,1447.55 1687.14,1447.55 1688.18,1447.55 1689.21,1447.55 \n",
       "  1690.25,1447.55 1691.29,1447.56 1692.32,1447.56 1693.36,1447.56 1694.4,1447.56 1695.43,1447.56 1696.47,1447.56 1697.51,1447.56 1698.54,1447.56 1699.58,1447.56 \n",
       "  1700.62,1447.56 1701.65,1447.56 1702.69,1447.56 1703.73,1447.56 1704.77,1447.56 1705.8,1447.57 1706.84,1447.57 1707.88,1447.57 1708.91,1447.57 1709.95,1447.57 \n",
       "  1710.99,1447.57 1712.02,1447.57 1713.06,1447.57 1714.1,1447.57 1715.13,1447.57 1716.17,1447.57 1717.21,1447.57 1718.24,1447.57 1719.28,1447.57 1720.32,1447.58 \n",
       "  1721.35,1447.58 1722.39,1447.58 1723.43,1447.58 1724.47,1447.58 1725.5,1447.58 1726.54,1447.58 1727.58,1447.58 1728.61,1447.58 1729.65,1447.58 1730.69,1447.58 \n",
       "  1731.72,1447.58 1732.76,1447.58 1733.8,1447.58 1734.83,1447.58 1735.87,1447.59 1736.91,1447.59 1737.94,1447.59 1738.98,1447.59 1740.02,1447.59 1741.05,1447.59 \n",
       "  1742.09,1447.59 1743.13,1447.59 1744.16,1447.59 1745.2,1447.59 1746.24,1447.59 1747.28,1447.59 1748.31,1447.59 1749.35,1447.59 1750.39,1447.6 1751.42,1447.6 \n",
       "  1752.46,1447.6 1753.5,1447.6 1754.53,1447.6 1755.57,1447.6 1756.61,1447.6 1757.64,1447.6 1758.68,1447.6 1759.72,1447.6 1760.75,1447.6 1761.79,1447.6 \n",
       "  1762.83,1447.6 1763.86,1447.6 1764.9,1447.6 1765.94,1447.61 1766.98,1447.61 1768.01,1447.61 1769.05,1447.61 1770.09,1447.61 1771.12,1447.61 1772.16,1447.61 \n",
       "  1773.2,1447.61 1774.23,1447.61 1775.27,1447.61 1776.31,1447.61 1777.34,1447.61 1778.38,1447.61 1779.42,1447.61 1780.45,1447.61 1781.49,1447.61 1782.53,1447.61 \n",
       "  1783.56,1447.61 1784.6,1447.61 1785.64,1447.62 1786.68,1447.62 1787.71,1447.62 1788.75,1447.62 1789.79,1447.62 1790.82,1447.62 1791.86,1447.62 1792.9,1447.62 \n",
       "  1793.93,1447.62 1794.97,1447.62 1796.01,1447.62 1797.04,1447.62 1798.08,1447.62 1799.12,1447.62 1800.15,1447.62 1801.19,1447.62 1802.23,1447.63 1803.26,1447.63 \n",
       "  1804.3,1447.63 1805.34,1447.63 1806.38,1447.63 1807.41,1447.63 1808.45,1447.63 1809.49,1447.63 1810.52,1447.63 1811.56,1447.63 1812.6,1447.63 1813.63,1447.63 \n",
       "  1814.67,1447.63 1815.71,1447.63 1816.74,1447.63 1817.78,1447.63 1818.82,1447.64 1819.85,1447.64 1820.89,1447.64 1821.93,1447.64 1822.96,1447.64 1824,1447.64 \n",
       "  1825.04,1447.64 1826.07,1447.64 1827.11,1447.64 1828.15,1447.64 1829.19,1447.64 1830.22,1447.64 1831.26,1447.64 1832.3,1447.64 1833.33,1447.64 1834.37,1447.65 \n",
       "  1835.41,1447.65 1836.44,1447.65 1837.48,1447.65 1838.52,1447.65 1839.55,1447.65 1840.59,1447.65 1841.63,1447.65 1842.66,1447.65 1843.7,1447.65 1844.74,1447.65 \n",
       "  1845.77,1447.65 1846.81,1447.65 1847.85,1447.65 1848.89,1447.65 1849.92,1447.65 1850.96,1447.65 1852,1447.65 1853.03,1447.65 1854.07,1447.65 1855.11,1447.65 \n",
       "  1856.14,1447.65 1857.18,1447.65 1858.22,1447.65 1859.25,1447.65 1860.29,1447.65 1861.33,1447.65 1862.36,1447.65 1863.4,1447.65 1864.44,1447.66 1865.47,1447.66 \n",
       "  1866.51,1447.66 1867.55,1447.66 1868.59,1447.66 1869.62,1447.66 1870.66,1447.66 1871.7,1447.66 1872.73,1447.66 1873.77,1447.66 1874.81,1447.66 1875.84,1447.66 \n",
       "  1876.88,1447.66 1877.92,1447.66 1878.95,1447.66 1879.99,1447.66 1881.03,1447.66 1882.06,1447.67 1883.1,1447.67 1884.14,1447.67 1885.17,1447.67 1886.21,1447.67 \n",
       "  1887.25,1447.67 1888.28,1447.67 1889.32,1447.67 1890.36,1447.67 1891.4,1447.67 1892.43,1447.67 1893.47,1447.67 1894.51,1447.67 1895.54,1447.67 1896.58,1447.67 \n",
       "  1897.62,1447.67 1898.65,1447.68 1899.69,1447.68 1900.73,1447.68 1901.76,1447.68 1902.8,1447.68 1903.84,1447.68 1904.87,1447.68 1905.91,1447.68 1906.95,1447.68 \n",
       "  1907.98,1447.68 1909.02,1447.68 1910.06,1447.68 1911.1,1447.68 1912.13,1447.68 1913.17,1447.68 1914.21,1447.69 1915.24,1447.69 1916.28,1447.69 1917.32,1447.69 \n",
       "  1918.35,1447.69 1919.39,1447.69 1920.43,1447.69 1921.46,1447.69 1922.5,1447.69 1923.54,1447.69 1924.57,1447.69 1925.61,1447.69 1926.65,1447.69 1927.68,1447.69 \n",
       "  1928.72,1447.69 1929.76,1447.69 1930.8,1447.7 1931.83,1447.7 1932.87,1447.7 1933.91,1447.7 1934.94,1447.7 1935.98,1447.7 1937.02,1447.69 1938.05,1447.7 \n",
       "  1939.09,1447.7 1940.13,1447.7 1941.16,1447.7 1942.2,1447.7 1943.24,1447.7 1944.27,1447.7 1945.31,1447.7 1946.35,1447.7 1947.38,1447.7 1948.42,1447.7 \n",
       "  1949.46,1447.7 1950.5,1447.7 1951.53,1447.7 1952.57,1447.7 1953.61,1447.7 1954.64,1447.7 1955.68,1447.7 1956.72,1447.7 1957.75,1447.7 1958.79,1447.7 \n",
       "  1959.83,1447.7 1960.86,1447.7 1961.9,1447.71 1962.94,1447.71 1963.97,1447.71 1965.01,1447.71 1966.05,1447.71 1967.08,1447.71 1968.12,1447.71 1969.16,1447.71 \n",
       "  1970.19,1447.71 1971.23,1447.71 1972.27,1447.71 1973.31,1447.71 1974.34,1447.71 1975.38,1447.71 1976.42,1447.71 1977.45,1447.71 1978.49,1447.71 1979.53,1447.72 \n",
       "  1980.56,1447.72 1981.6,1447.72 1982.64,1447.72 1983.67,1447.72 1984.71,1447.72 1985.75,1447.72 1986.78,1447.72 1987.82,1447.72 1988.86,1447.72 1989.89,1447.72 \n",
       "  1990.93,1447.72 1991.97,1447.72 1993.01,1447.72 1994.04,1447.72 1995.08,1447.72 1996.12,1447.73 1997.15,1447.73 1998.19,1447.73 1999.23,1447.73 2000.26,1447.73 \n",
       "  2001.3,1447.73 2002.34,1447.73 2003.37,1447.73 2004.41,1447.73 2005.45,1447.73 2006.48,1447.73 2007.52,1447.73 2008.56,1447.73 2009.59,1447.73 2010.63,1447.73 \n",
       "  2011.67,1447.73 2012.71,1447.73 2013.74,1447.74 2014.78,1447.74 2015.82,1447.74 2016.85,1447.74 2017.89,1447.74 2018.93,1447.74 2019.96,1447.74 2021,1447.74 \n",
       "  2022.04,1447.74 2023.07,1447.74 2024.11,1447.74 2025.15,1447.74 2026.18,1447.74 2027.22,1447.74 2028.26,1447.74 2029.29,1447.74 2030.33,1447.75 2031.37,1447.75 \n",
       "  2032.41,1447.75 2033.44,1447.75 2034.48,1447.75 2035.52,1447.75 2036.55,1447.75 2037.59,1447.75 2038.63,1447.75 2039.66,1447.75 2040.7,1447.75 2041.74,1447.75 \n",
       "  2042.77,1447.75 2043.81,1447.75 2044.85,1447.75 2045.88,1447.75 2046.92,1447.76 2047.96,1447.76 2048.99,1447.76 2050.03,1447.76 2051.07,1447.76 2052.1,1447.76 \n",
       "  2053.14,1447.76 2054.18,1447.76 2055.22,1447.76 2056.25,1447.76 2057.29,1447.76 2058.33,1447.76 2059.36,1447.76 2060.4,1447.76 2061.44,1447.76 2062.47,1447.76 \n",
       "  2063.51,1447.76 2064.55,1447.77 2065.58,1447.77 2066.62,1447.77 2067.66,1447.77 2068.69,1447.77 2069.73,1447.77 2070.77,1447.77 2071.8,1447.77 2072.84,1447.77 \n",
       "  2073.88,1447.77 2074.92,1447.77 2075.95,1447.77 2076.99,1447.77 2078.03,1447.77 2079.06,1447.77 2080.1,1447.77 2081.14,1447.77 2082.17,1447.78 2083.21,1447.78 \n",
       "  2084.25,1447.78 2085.28,1447.78 2086.32,1447.78 2087.36,1447.78 2088.39,1447.78 2089.43,1447.78 2090.47,1447.78 2091.5,1447.78 2092.54,1447.78 2093.58,1447.78 \n",
       "  2094.62,1447.78 2095.65,1447.78 2096.69,1447.78 2097.73,1447.78 2098.76,1447.79 2099.8,1447.79 2100.84,1447.79 2101.87,1447.79 2102.91,1447.79 2103.95,1447.79 \n",
       "  2104.98,1447.79 2106.02,1447.79 2107.06,1447.79 2108.09,1447.79 2109.13,1447.79 2110.17,1447.79 2111.2,1447.79 2112.24,1447.79 2113.28,1447.79 2114.31,1447.79 \n",
       "  2115.35,1447.79 2116.39,1447.79 2117.43,1447.79 2118.46,1447.79 2119.5,1447.79 2120.54,1447.79 2121.57,1447.79 2122.61,1447.79 2123.65,1447.8 2124.68,1447.8 \n",
       "  2125.72,1447.8 2126.76,1447.8 2127.79,1447.8 2128.83,1447.8 2129.87,1447.8 2130.9,1447.8 2131.94,1447.8 2132.98,1447.8 2134.01,1447.8 2135.05,1447.8 \n",
       "  2136.09,1447.8 2137.13,1447.8 2138.16,1447.8 2139.2,1447.8 2140.24,1447.8 2141.27,1447.8 2142.31,1447.8 2143.35,1447.81 2144.38,1447.81 2145.42,1447.81 \n",
       "  2146.46,1447.81 2147.49,1447.81 2148.53,1447.81 2149.57,1447.81 2150.6,1447.81 2151.64,1447.81 2152.68,1447.81 2153.71,1447.81 2154.75,1447.81 2155.79,1447.81 \n",
       "  2156.83,1447.81 2157.86,1447.81 2158.9,1447.81 2159.94,1447.81 2160.97,1447.81 2162.01,1447.82 2163.05,1447.82 2164.08,1447.82 2165.12,1447.82 2166.16,1447.82 \n",
       "  2167.19,1447.82 2168.23,1447.82 2169.27,1447.82 2170.3,1447.82 2171.34,1447.82 2172.38,1447.82 2173.41,1447.82 2174.45,1447.82 2175.49,1447.82 2176.53,1447.82 \n",
       "  2177.56,1447.82 2178.6,1447.82 2179.64,1447.83 2180.67,1447.83 2181.71,1447.83 2182.75,1447.83 2183.78,1447.83 2184.82,1447.83 2185.86,1447.83 2186.89,1447.83 \n",
       "  2187.93,1447.83 2188.97,1447.83 2190,1447.83 2191.04,1447.83 2192.08,1447.83 2193.11,1447.83 2194.15,1447.83 2195.19,1447.83 2196.22,1447.83 2197.26,1447.83 \n",
       "  2198.3,1447.84 2199.34,1447.84 2200.37,1447.84 2201.41,1447.84 2202.45,1447.84 2203.48,1447.84 2204.52,1447.84 2205.56,1447.84 2206.59,1447.84 2207.63,1447.84 \n",
       "  2208.67,1447.84 2209.7,1447.84 2210.74,1447.84 2211.78,1447.84 2212.81,1447.84 2213.85,1447.84 2214.89,1447.84 2215.92,1447.84 2216.96,1447.84 2218,1447.84 \n",
       "  2219.04,1447.84 2220.07,1447.84 2221.11,1447.84 2222.15,1447.84 2223.18,1447.84 2224.22,1447.84 2225.26,1447.84 2226.29,1447.84 2227.33,1447.84 2228.37,1447.84 \n",
       "  2229.4,1447.84 2230.44,1447.84 2231.48,1447.84 2232.51,1447.84 2233.55,1447.84 2234.59,1447.84 2235.62,1447.84 2236.66,1447.85 2237.7,1447.85 2238.74,1447.85 \n",
       "  2239.77,1447.85 2240.81,1447.85 2241.85,1447.85 2242.88,1447.85 2243.92,1447.85 2244.96,1447.85 2245.99,1447.85 2247.03,1447.85 2248.07,1447.85 2249.1,1447.85 \n",
       "  2250.14,1447.85 2251.18,1447.85 2252.21,1447.85 2253.25,1447.85 2254.29,1447.85 2255.32,1447.85 2256.36,1447.86 2257.4,1447.86 2258.44,1447.86 2259.47,1447.86 \n",
       "  2260.51,1447.86 2261.55,1447.86 2262.58,1447.86 2263.62,1447.86 2264.66,1447.86 2265.69,1447.86 2266.73,1447.86 2267.77,1447.86 2268.8,1447.86 2269.84,1447.86 \n",
       "  2270.88,1447.86 2271.91,1447.86 2272.95,1447.86 2273.99,1447.86 2275.02,1447.87 2276.06,1447.87 2277.1,1447.87 2278.13,1447.87 2279.17,1447.87 2280.21,1447.87 \n",
       "  2281.25,1447.87 2282.28,1447.87 2283.32,1447.87 2284.36,1447.87 2285.39,1447.87 2286.43,1447.87 2287.47,1447.87 2288.5,1447.87 2289.54,1447.87 2290.58,1447.87 \n",
       "  \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnn_model2 = Chain( RNN( x_dim => h_dim ) , Dense( h_dim => y_dim , relu) , softmax );\n",
    "data = Flux.DataLoader( (x_train, y_train) , batchsize=10 , shuffle=false );\n",
    "opt = Flux.setup( Flux.Adam(0.01) , rnn_model2 );\n",
    "\n",
    "losses = []\n",
    "for epoch in 1:2000\n",
    "    for ( x , y ) in data\n",
    "\n",
    "        #doing this since we added the data as arrays instead of a matrix                \n",
    "        loss_tmp, grads = Flux.withgradient( rnn_model2 ) do model\n",
    "            loss = 0\n",
    "            Flux.reset!(model) #important or else the model takes the 'hidden' values from previous usage \n",
    "            for (xi,yi) in zip(x,y)\n",
    "                      \n",
    "                y_hat = model(xi)\n",
    "#                 println(y_hat)\n",
    "                loss += Flux.mse(y_hat, yi)\n",
    "            end\n",
    "            return loss\n",
    "        end         \n",
    "        Flux.update!( opt , rnn_model2 , grads[1] )\n",
    "        push!(losses,loss_tmp) #don't put this in the above block where the gradient is computed\n",
    "    end\n",
    "    if( epoch == 250 ) Flux.adjust!( opt , 0.001 ) end\n",
    "    if( epoch == 500 ) Flux.adjust!( opt , 0.0005 ) end\n",
    "    #break\n",
    "end\n",
    "p1 = plot( losses , title=\"RNN food training loss\" , legend=false )\n",
    "savefig(p1,\"p1.png\")\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y Any[\"soup\", \"steak\", \"soup\", \"soup\"]\n",
      "yhat [\"soup\", \"steak\", \"soup\", \"soup\"]\n",
      "--------------------------------\n",
      "y Any[\"soup\", \"steak\", \"chicken\", \"soup\"]\n",
      "yhat [\"soup\", \"steak\", \"steak\", \"soup\"]\n",
      "--------------------------------\n",
      "y Any[\"steak\", \"chicken\", \"soup\", \"steak\"]\n",
      "yhat [\"steak\", \"steak\", \"soup\", \"steak\"]\n",
      "--------------------------------\n",
      "y Any[\"steak\", \"soup\", \"steak\", \"chicken\"]\n",
      "yhat [\"steak\", \"soup\", \"steak\", \"steak\"]\n",
      "--------------------------------\n",
      "y Any[\"soup\", \"steak\", \"chicken\", \"chicken\"]\n",
      "yhat [\"soup\", \"steak\", \"steak\", \"steak\"]\n",
      "--------------------------------\n",
      "y Any[\"steak\", \"soup\", \"soup\", \"steak\"]\n",
      "yhat [\"steak\", \"soup\", \"soup\", \"steak\"]\n",
      "--------------------------------\n",
      "y Any[\"steak\", \"chicken\", \"chicken\", \"chicken\"]\n",
      "yhat [\"steak\", \"steak\", \"steak\", \"steak\"]\n",
      "--------------------------------\n",
      "y Any[\"steak\", \"soup\", \"soup\", \"soup\"]\n",
      "yhat [\"steak\", \"soup\", \"soup\", \"soup\"]\n",
      "--------------------------------\n",
      "y Any[\"steak\", \"chicken\", \"soup\", \"soup\"]\n",
      "yhat [\"steak\", \"steak\", \"soup\", \"soup\"]\n",
      "--------------------------------\n",
      "y Any[\"steak\", \"chicken\", \"chicken\", \"soup\"]\n",
      "yhat [\"steak\", \"steak\", \"steak\", \"soup\"]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for (x,y) in zip(x_test,y_test_cold)\n",
    "    Flux.reset!(rnn_model2)\n",
    "    yhat = Flux.onecold(rnn_model2(x),y_categories)\n",
    "    println(\"y \", y)\n",
    "    println(\"yhat \", yhat)\n",
    "    println(\"--------------------------------\")\n",
    "end\n",
    "# !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.codeproject.com%2FArticles%2F3993967%2FApplying-Long-Short-Term-Memory-for-Video-Classifi&psig=AOvVaw3aJjMa8pFFZ1i53Lk2C-gE&ust=1673466988547000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCNjijr7kvfwCFQAAAAAdAAAAABA4)\n",
    "\n",
    "![rnn](./rnnTypes.jpg)\n",
    "\n",
    "#### Let's consider the <u>**sequence to one**</u> (seq-to-one) now\n",
    "\n",
    "#### Many-to-One Sequence Problems. In many-to-one sequence problems, we have a sequence of data as input, and we have to predict a single output. Sentiment analysis or text classification is one such use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=Float32[0.24803472] x=Vector{Float32}[[0.7601079, 0.84414154], [0.84445024, 0.28849977], [0.449445, 0.9851977]]\n",
      "Float32[1.4257019]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3869f0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#loss function for 3-to-1, a sequence of 3 inputs for a single output\n",
    "#this can be used in the training scheme as before\n",
    "function loss_3_to_1( x , y ) #assume feature data has 3 samples\n",
    "    rnn_model1( x[1] ) # ignores the output but updates the hidden states\n",
    "    rnn_model1( x[2] ) # ignores the output but updates the hidden states again\n",
    "    y_hat = rnn_model1( x[3] )\n",
    "    println( y_hat )\n",
    "    Flux.mse( y_hat , y )\n",
    "end\n",
    "\n",
    "y = rand( Float32 , y_dim ) #target data\n",
    "x = [ rand(Float32, x_dim ) for i=1:3 ] #sequence of 3 x values\n",
    "println(\"y=\",y,\" x=\",x)\n",
    "loss_3_to_1( x , y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[1.9506261]\n",
      "loss = 1.1709299\n",
      "Float32[1.5288637]\n",
      "loss = 0.28548747\n",
      "Float32[1.3258337]\n",
      "loss = 1.6201891\n",
      "Float32[1.6926062]\n",
      "loss = 2.0465567\n",
      "Float32[1.7179067]\n",
      "loss = 0.5677387\n",
      "Float32[0.9220013]\n",
      "loss = 0.3709042\n",
      "Float32[1.5273948]\n",
      "loss = 1.9376503\n",
      "Float32[1.3230592]\n",
      "loss = 0.8318744\n",
      "Float32[1.7548747]\n",
      "loss = 0.6737301\n",
      "Float32[1.2160401]\n",
      "loss = 0.16275056\n"
     ]
    }
   ],
   "source": [
    "#produce a hypothetical sequence of data points in x_dim dimensions\n",
    "#with y_dim data and pass that to the loss\n",
    "x_data = [ [rand(Float32,x_dim) for i=1:3] for j=1:10 ]\n",
    "y_data = [ rand( Float32 , y_dim ) for j=1:10 ]\n",
    "data = zip( x_data , y_data ) #pack the data into pairs\n",
    "\n",
    "for (x_tmp,y_tmp) in data\n",
    "    println( \"loss = \" , loss_3_to_1( x_tmp , y_tmp ) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_3_to_1_reset (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add a reset so that each sentence is taken independently\n",
    "function loss_3_to_1_reset( x , y ) #assume feature data has 3 samples\n",
    "    Flux.reset!( rnn_model1 ) #reset the model from previous sentences\n",
    "    rnn_model1( x[1] ) # ignores the output but updates the hidden states\n",
    "    rnn_model1( x[2] ) # ignores the output but updates the hidden states again\n",
    "    y_hat = rnn_model1( x[3] )\n",
    "    Flux.mse( y_hat , y )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">One to many RNN</span>\n",
    "\n",
    "- Take a single X input and then output a series of Y outputs\n",
    "- Some architectures will require a bit of manual construction\n",
    "- One-to-many sequence problems are sequence problems where the input data has one time-step, and the output contains a vector of multiple values or multiple time-steps. Thus, we have a single input and a sequence of outputs.\n",
    "A typical example is image captioning, where the description of an image is generated. Or music generation from a description.\n",
    "\n",
    "[link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fone-to-many-lstm%2F96932&psig=AOvVaw0Ha6GO6T8UyXiD-It1widc&ust=1673625753189000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCMie7vWzwvwCFQAAAAAdAAAAABBa)\n",
    "\n",
    "![rnn](./rnn1toMany.jpeg)\n",
    "\n",
    "[link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fone-to-many-lstm%2F96932&psig=AOvVaw0Ha6GO6T8UyXiD-It1widc&ust=1673625753189000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCMie7vWzwvwCFQAAAAAdAAAAABBa)\n",
    "\n",
    "![rnn](./one-to-many.png)\n",
    "\n",
    "### In this case we must make sure the 'hidden' state values are cycled as inputs and the previous unit definitions abstract away that hidden unit productions. The units must return the states (hidden) so be passed as subsequent inputs.\n",
    "\n",
    "#### instead of having Flux handle the internal state (h) we can maintain it explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.23961921, 0.5914116], Float32[0.23961921, 0.5914116])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a RNN cell which does not handle internal state automatically\n",
    "#which means that we have to retrieve the 'h' state value and pass it back in to the next step\n",
    "x_dim = 2\n",
    "h_dim = 2\n",
    "h_init = rand( Float32 , h_dim )\n",
    "x_init = rand( Float32 , x_dim )\n",
    "\n",
    "rnn1 = Flux.RNNCell( x_dim, h_dim, sigmoid ) #define x_dim and then h_dim\n",
    "\n",
    "h , y = rnn1( h_init , x_init ) #pass h and then x to get the h and y outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 0.6980755"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we pass the outputs 'y' from the rnn units as 'x' data in subsequent steps\n",
    "x_dim = 2\n",
    "h_dim = 2\n",
    "h_init = rand( Float32 , h_dim )\n",
    "x_init = rand( Float32 , x_dim )\n",
    "\n",
    "rnn1 = Flux.RNNCell( x_dim, h_dim, sigmoid ) #define x_dim and then h_dim\n",
    "m1 = Flux.Dense( 2 => 1 , sigmoid )\n",
    "\n",
    "h , y = rnn1( h_init , x_init ) #pass h and then x to get the h and y outputs\n",
    "\n",
    "h , y = rnn1( h , y ) #pass h and then x to get the h and y outputs\n",
    "h , y = rnn1( h , y ) #pass h and then x to get the h and y outputs\n",
    "h , y = rnn1( h , y ) #pass h and then x to get the h and y outputs\n",
    "y_hat = m1( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Matrix{Float32}}:\n",
       " [0.7193387 0.33868003; 0.016090214 0.256925; 0.2488938 0.47927523]\n",
       " [0.56273735 0.5567441; 0.20874453 0.45442927; 0.6528861 0.7265376]\n",
       " [0.5801285 0.41532242; 0.91156983 0.92337066; 0.66897005 0.5273158]\n",
       " [0.73792315 0.7799565; 0.5407746 0.5092; 0.6379438 0.23041004]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       " -0.344579  -0.125573\n",
       " -0.119549  -0.341085\n",
       "  0.22776    0.012745\n",
       " -0.451451  -0.568937\n",
       "  0.337278  -0.00986344"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       "  0.138712    0.112828\n",
       " -0.326718   -0.57339\n",
       " -0.0753149  -0.490038\n",
       " -0.862643   -0.845221\n",
       "  0.167302    0.156598"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       "  0.477808   0.602143\n",
       " -0.618007  -0.544691\n",
       " -0.528367  -0.617153\n",
       " -0.925458  -0.913587\n",
       "  0.371714   0.403402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       "  0.738152   0.738869\n",
       " -0.659737  -0.444979\n",
       " -0.471581  -0.31129\n",
       " -0.907443  -0.83823\n",
       "  0.788016   0.860447"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Nothing}:\n",
       " nothing\n",
       " nothing\n",
       " nothing\n",
       " nothing"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsteps = 4\n",
    "xs = [rand(Float32, 3, 2) for i in 1:nsteps]\n",
    "display( xs )\n",
    "m = Chain(RNN(3, 5))\n",
    "[ display(m(x)) for x = xs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <span style=\"color:orange\">write a function and make a function for the gradient of the function</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
