{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Recurrent Models with FluxML</span>\n",
    "\n",
    "#### <span style=\"color:orange\"> Flux does offer out of the box a set of recurrence functionalities in specific layers. Remember that recurrent models can come across as being more complicated than necessary. In general we are still dealing with the same type of funcational relationship, $\\hat{y} = f(X_i) = f_{rnn}(X_t)$ where previously y_hat was either a single dimension or multiple dimensions, here $y_{hat} = [y_t , h_t]$ where $h_t$ is an input into the new time point (memory carry on) so that we have $X_t = [ x_t , h_{t-1}]$. The dependency can be seen as $\\hat{y}_t = f(x_t,h_{t-1}), h_t = g(h_{t-1},x_t)$ </span>\n",
    "\n",
    "- This basic recurrence relationship says that at each point we take the $h_{t-1}$ from the previous time step 't-1', we also use the current inputs at time t, $x_t$ and then produce an output which is has 2 components, $y_t$ and $h_t$ (where $h_t$ feeds into $t+1$). If we focus on a single time point, we are still doing the functional mapping that we had before with the chain of Dense layers.\n",
    "\n",
    "- The below image [link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fswlh%2Fintroduction-to-recurrent-neural-networks-rnns-347903dd8d81&psig=AOvVaw3xmMdMdDizNUUWXy021QUO&ust=1673451409556000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCOCStLiqvfwCFQAAAAAdAAAAABA_) shows how this looks in a model diagram for **seq-to-seq** (**many to many**)\n",
    "\n",
    "![rnn](./rnn1.png)\n",
    "\n",
    "- also The below image [link source](https://www.ibm.com/topics/recurrent-neural-networks) shows how this looks\n",
    "\n",
    "![rnn](./rnn2.jpeg)\n",
    "\n",
    "----------------\n",
    "\n",
    "#### <span style=\"color:orange\"> $W_h$ is the weight matrix (tranformation) on the 'hidden inputs' $h_{t-1}$ that come from the previous unit's 'hidden' output, $W_x$ is the weight matrix (transformation) upon the inputs at the current time $x_t$. $W_y$ is the transformation weight matrix applied to the 'current' hidden value produced from the cell that after a non-linear transformation (activation function) produces the output $\\hat{y}_t$. Training involves learning the values of the weights/parameters for these matrices.</span>\n",
    "\n",
    "### <span style=\"color:orange\"> $h_t = tahn(b_h + W_h^t h_{t-1} + W_x^t x_t)$ </span>\n",
    "### <span style=\"color:orange\"> $\\hat{y}_t = softmax(b_y + W_{y}^{t} h_t)$ </span>\n",
    "\n",
    "RNN can bring to mind the Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/.julia/environments/v1.8/Project.toml`\n",
      " \u001b[90m [587475ba] \u001b[39mFlux v0.13.11\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.status(\"Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using Plots\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.5169814, 0.91928077, -0.9481162, -0.985066, -0.6387447]\n"
     ]
    }
   ],
   "source": [
    "#example of usage of the RNN unit\n",
    "h_dim = 5 #hidden dimension\n",
    "x_dim = 2 #input dimension at time t\n",
    "y_dim = 1 #the output dimension that 'we' see\n",
    "\n",
    "rnn_tmp = RNN( x_dim , h_dim ) #produces the cell\n",
    "x_t1 = Float32.( [1,2] ) #some arbitrary input data\n",
    "println( rnn_tmp( x_t1 ) ) #print the output h_t1 from the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> It is key to know that this cell is different from the Dense layers in that it maintains the state between executions since it is **stateful**. This means it holds the state via a closure inside the function reference</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.94987404, 0.64039564, -0.985474, -0.95143384, 0.5043771]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Vector{Float32}}:\n",
       " [0.9456798, -0.011277318, -0.86470556, -0.9337519, 0.23209512]\n",
       " [0.97272575, 0.46123296, -0.85870326, -0.9625054, 0.10129218]\n",
       " [0.9610443, 0.38343206, -0.91787034, -0.94277394, 0.33973595]\n",
       " [0.9594942, 0.2358451, -0.8726213, -0.9461099, 0.21138343]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_t1 = Float32.( [1,2] ) #some arbitrary input data\n",
    "println( rnn_tmp( x_t1 ) ) #print the output h_t1 from the cell\n",
    "#print multiple times to see the changes\n",
    "display( [ rnn_tmp( x_t1 ) for _ in 1:4 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> Since the RNN unit maintains and handles the state between subsequent uses we can abstractly use it in the ML pipeline as we used the Dense layer. From above notice that we produced hidden representation responses from inputs, but not the predictions $\\hat{y}$ since those are done separately. </span>\n",
    "\n",
    "### <span style=\"color:orange\"> The RNN function implements $h_t = tahn(b_h + W_h^t h_{t-1} + W_x^t x_t)$ but $\\hat{y}_t = softmax(b_y + W_{y}^{t} h_t)$ is not. The $W_{y}$ matrix is not provided by the RNN layer and must be supplied by the user. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Recur(\n",
       "    RNNCell(2 => 5, tanh),              \u001b[90m# 45 parameters\u001b[39m\n",
       "  ),\n",
       "  Dense(5 => 1),                        \u001b[90m# 6 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: 6 trainable arrays, \u001b[39m51 parameters,\n",
       "\u001b[90m          # plus 1 non-trainable, 5 parameters, summarysize \u001b[39m580 bytes."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we feed the model with 'x_dim' data, it produces a hidden vector 'h_dim' and outputs a 'y_dim' vector at each time\n",
    "rnn_model1 = Chain( RNN( x_dim => h_dim ) , Dense( h_dim => y_dim ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " -0.36381838"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try out the model\n",
    "rnn_model1( x_t1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Vector{Float32}}:\n",
       " [0.14091158]\n",
       " [0.42123732]\n",
       " [0.53046966]\n",
       " [0.57932866]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ rnn_model1( x_t1 ) for _ in 1:4 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Vector{Float32}}:\n",
       " [0.61494243]\n",
       " [0.6501855]\n",
       " [0.5069595]\n",
       " [0.650578]\n",
       " [0.57627857]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the model with hypothetical data\n",
    "x_length = 5\n",
    "#generate some random data as inputs, to be treated as a sequence\n",
    "x_seq = [ rand( Float32 , x_dim ) for i = 1:x_length ] #sequence data\n",
    "[ rnn_model1( xt ) for xt in x_seq ] #predicted y_t data from the RNN\n",
    "#this is <sequence to sequence> <many to many>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\"> If you need to use the RNN and not have it dependent upon the previous state (eg. independent sentences), then you can use the **Flux.reset!(rnn_model)** command so the previous history variables are removed </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Recur(\n",
       "    RNNCell(1 => 3, tanh),              \u001b[90m# 18 parameters\u001b[39m\n",
       "  ),\n",
       "  Dense(3 => 3),                        \u001b[90m# 12 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: 6 trainable arrays, \u001b[39m30 parameters,\n",
       "\u001b[90m          # plus 1 non-trainable, 3 parameters, summarysize \u001b[39m496 bytes."
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_dim = 3 #hidden dimension\n",
    "x_dim = 1 #input dimension at time t\n",
    "y_dim = 3 #the output dimension that 'we' see\n",
    "sequence_length = 5\n",
    "rnn_model2 = Chain( RNN( x_dim => h_dim ) , Dense( h_dim => y_dim ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[-0.027664214, 0.4412998, -0.37990218]\n",
      "Float32[-0.027664214, 0.4412998, -0.37990218]\n",
      "Float32[-0.027664214, 0.4412998, -0.37990218]\n",
      "Float32[-0.027664214, 0.4412998, -0.37990218]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"----updating (no reset)----\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[0.2818488, 0.013466537, 0.1102349]\n",
      "Float32[-0.1665119, 0.4286871, -0.44341806]\n",
      "Float32[0.3505441, 0.060505107, 0.09921164]\n",
      "Float32[-0.17685528, 0.38966638, -0.41995186]\n"
     ]
    }
   ],
   "source": [
    "#no updating via reset which resets the RNN hidden state\n",
    "x_tmp = rand( Float32 , x_dim )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "Flux.reset!(rnn_model2)\n",
    "println( rnn_model2(x_tmp) )\n",
    "#updating\n",
    "display( \"----updating (no reset)----\" )\n",
    "println( rnn_model2(x_tmp) )\n",
    "println( rnn_model2(x_tmp) )\n",
    "println( rnn_model2(x_tmp) )\n",
    "println( rnn_model2(x_tmp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start a new model fresh again\n",
    "x_categories = [ \"happy\" , \"sad\" ]\n",
    "y_categories = [ \"steak\" , \"chicken\" , \"soup\" ]\n",
    "h_dim = 3 #hidden dimension\n",
    "x_dim = length( x_categories ) #input dimension at time t\n",
    "y_dim = length( y_categories ) #the output dimension that 'we' see\n",
    "sequence_length = 5\n",
    "\n",
    "rnn_model2 = Chain( RNN( x_dim => h_dim ) , Dense( h_dim => y_dim ) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (make some data on food choices) dependency is that when happy is seen: steak, then on subsequent happy: chicken, upon sad: soup (constantly)\n",
    "\n",
    "the key is here that there should be memory of having seen sad which overrides newer observations of happy so that the menu serves soup regardless and that the RNN stores this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"happy\", \"sad\", \"happy\", \"happy\", \"happy\"]\n",
      "Any[\"steak\", \"soup\", \"soup\", \"soup\", \"soup\"]\n"
     ]
    }
   ],
   "source": [
    "function RNN_X_Data_Food( sequence_length )\n",
    "    #rand(x_categories,sequence_length)\n",
    "    return x_categories[ rand(Categorical([0.7,0.3]),sequence_length) ] #higher prob of happy\n",
    "end\n",
    "x_seq_cold = RNN_X_Data_Food( sequence_length )\n",
    "println( x_seq_cold )\n",
    "#dependency is that when happy is seen: steak, then on subsequent happy: chicken, upon sad: soup (constantly)\n",
    "function RNN_Y_Data_Food( x_seq_cold )\n",
    "    y_seq_cold = []\n",
    "    prev_xx = \"\"\n",
    "    seen_sad = false\n",
    "    for xx in x_seq_cold\n",
    "        if( seen_sad == false && prev_xx != \"happy\" && xx == \"happy\" )\n",
    "            push!( y_seq_cold , \"steak\" )\n",
    "        elseif( seen_sad == false && prev_xx == \"happy\" && xx == \"happy\" )\n",
    "            push!( y_seq_cold , \"chicken\" )\n",
    "        else\n",
    "            push!( y_seq_cold , \"soup\" )\n",
    "            seen_sad = true\n",
    "        end\n",
    "        prev_xx = xx\n",
    "    end\n",
    "    return y_seq_cold\n",
    "end\n",
    "println( RNN_Y_Data_Food(x_seq_cold) ) #output food agenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = 200\n",
    "#generate the stochastic data upon the rule set defined in the functions\n",
    "x_train_cold = [ RNN_X_Data_Food(sequence_length) for _ in 1:NN ]\n",
    "y_train_cold = RNN_Y_Data_Food.(x_train_cold)\n",
    "x_test_cold = [ RNN_X_Data_Food(sequence_length) for _ in 1:NN ]\n",
    "y_test_cold = RNN_Y_Data_Food.(x_test_cold);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the label data into one-hot-batch \n",
    "x_train = [ Flux.onehotbatch( x_train_cold[ii] , x_categories ) for ii in 1:length(x_train_cold) ]\n",
    "x_test = [ Flux.onehotbatch( x_test_cold[ii] , x_categories ) for ii in 1:length(x_test_cold) ];\n",
    "y_train = [ Flux.onehotbatch( y_train_cold[ii] , y_categories ) for ii in 1:length(y_train_cold) ];\n",
    "y_test = [ Flux.onehotbatch( y_test_cold[ii] , y_categories ) for ii in 1:length(y_test_cold) ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bool[1 0 1 1 0; 0 1 0 0 1]\n",
      "Bool[1 0 0 0 0; 0 0 0 0 0; 0 1 1 1 1]\n",
      "Float32[-0.1322509 0.18939666 -0.1322509 -0.1322509 0.18939666; -0.37909815 -0.05972867 -0.37909815 -0.37909815 -0.05972867; 0.600493 -0.11260858 0.600493 0.600493 -0.11260858]\n"
     ]
    }
   ],
   "source": [
    "println( x_train[1] )\n",
    "println( y_train[1] )\n",
    "y_hat = rnn_model2( x_train[1] ) #inspect what the model predicts given some training data (not trained yet)\n",
    "println( y_hat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_RNN_Food( x , y )\n",
    "    rnn_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.codeproject.com%2FArticles%2F3993967%2FApplying-Long-Short-Term-Memory-for-Video-Classifi&psig=AOvVaw3aJjMa8pFFZ1i53Lk2C-gE&ust=1673466988547000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCNjijr7kvfwCFQAAAAAdAAAAABA4)\n",
    "\n",
    "![rnn](./rnnTypes.jpg)\n",
    "\n",
    "#### Let's consider the <u>**sequence to one**</u> (seq-to-one) now\n",
    "\n",
    "#### Many-to-One Sequence Problems. In many-to-one sequence problems, we have a sequence of data as input, and we have to predict a single output. Sentiment analysis or text classification is one such use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=Float32[0.24803472] x=Vector{Float32}[[0.7601079, 0.84414154], [0.84445024, 0.28849977], [0.449445, 0.9851977]]\n",
      "Float32[1.4257019]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3869f0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#loss function for 3-to-1, a sequence of 3 inputs for a single output\n",
    "#this can be used in the training scheme as before\n",
    "function loss_3_to_1( x , y ) #assume feature data has 3 samples\n",
    "    rnn_model1( x[1] ) # ignores the output but updates the hidden states\n",
    "    rnn_model1( x[2] ) # ignores the output but updates the hidden states again\n",
    "    y_hat = rnn_model1( x[3] )\n",
    "    println( y_hat )\n",
    "    Flux.mse( y_hat , y )\n",
    "end\n",
    "\n",
    "y = rand( Float32 , y_dim ) #target data\n",
    "x = [ rand(Float32, x_dim ) for i=1:3 ] #sequence of 3 x values\n",
    "println(\"y=\",y,\" x=\",x)\n",
    "loss_3_to_1( x , y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[1.9506261]\n",
      "loss = 1.1709299\n",
      "Float32[1.5288637]\n",
      "loss = 0.28548747\n",
      "Float32[1.3258337]\n",
      "loss = 1.6201891\n",
      "Float32[1.6926062]\n",
      "loss = 2.0465567\n",
      "Float32[1.7179067]\n",
      "loss = 0.5677387\n",
      "Float32[0.9220013]\n",
      "loss = 0.3709042\n",
      "Float32[1.5273948]\n",
      "loss = 1.9376503\n",
      "Float32[1.3230592]\n",
      "loss = 0.8318744\n",
      "Float32[1.7548747]\n",
      "loss = 0.6737301\n",
      "Float32[1.2160401]\n",
      "loss = 0.16275056\n"
     ]
    }
   ],
   "source": [
    "#produce a hypothetical sequence of data points in x_dim dimensions\n",
    "#with y_dim data and pass that to the loss\n",
    "x_data = [ [rand(Float32,x_dim) for i=1:3] for j=1:10 ]\n",
    "y_data = [ rand( Float32 , y_dim ) for j=1:10 ]\n",
    "data = zip( x_data , y_data ) #pack the data into pairs\n",
    "\n",
    "for (x_tmp,y_tmp) in data\n",
    "    println( \"loss = \" , loss_3_to_1( x_tmp , y_tmp ) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_3_to_1_reset (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add a reset so that each sentence is taken independently\n",
    "function loss_3_to_1_reset( x , y ) #assume feature data has 3 samples\n",
    "    Flux.reset!( rnn_model1 ) #reset the model from previous sentences\n",
    "    rnn_model1( x[1] ) # ignores the output but updates the hidden states\n",
    "    rnn_model1( x[2] ) # ignores the output but updates the hidden states again\n",
    "    y_hat = rnn_model1( x[3] )\n",
    "    Flux.mse( y_hat , y )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">One to many RNN</span>\n",
    "\n",
    "- Take a single X input and then output a series of Y outputs\n",
    "- Some architectures will require a bit of manual construction\n",
    "- One-to-many sequence problems are sequence problems where the input data has one time-step, and the output contains a vector of multiple values or multiple time-steps. Thus, we have a single input and a sequence of outputs.\n",
    "A typical example is image captioning, where the description of an image is generated. Or music generation from a description.\n",
    "\n",
    "[link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fone-to-many-lstm%2F96932&psig=AOvVaw0Ha6GO6T8UyXiD-It1widc&ust=1673625753189000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCMie7vWzwvwCFQAAAAAdAAAAABBa)\n",
    "\n",
    "![rnn](./rnn1toMany.jpeg)\n",
    "\n",
    "[link source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fone-to-many-lstm%2F96932&psig=AOvVaw0Ha6GO6T8UyXiD-It1widc&ust=1673625753189000&source=images&cd=vfe&ved=0CBAQjhxqFwoTCMie7vWzwvwCFQAAAAAdAAAAABBa)\n",
    "\n",
    "![rnn](./one-to-many.png)\n",
    "\n",
    "### In this case we must make sure the 'hidden' state values are cycled as inputs and the previous unit definitions abstract away that hidden unit productions. The units must return the states (hidden) so be passed as subsequent inputs.\n",
    "\n",
    "#### instead of having Flux handle the internal state (h) we can maintain it explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.23961921, 0.5914116], Float32[0.23961921, 0.5914116])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a RNN cell which does not handle internal state automatically\n",
    "#which means that we have to retrieve the 'h' state value and pass it back in to the next step\n",
    "x_dim = 2\n",
    "h_dim = 2\n",
    "h_init = rand( Float32 , h_dim )\n",
    "x_init = rand( Float32 , x_dim )\n",
    "\n",
    "rnn1 = Flux.RNNCell( x_dim, h_dim, sigmoid ) #define x_dim and then h_dim\n",
    "\n",
    "h , y = rnn1( h_init , x_init ) #pass h and then x to get the h and y outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 0.6980755"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we pass the outputs 'y' from the rnn units as 'x' data in subsequent steps\n",
    "x_dim = 2\n",
    "h_dim = 2\n",
    "h_init = rand( Float32 , h_dim )\n",
    "x_init = rand( Float32 , x_dim )\n",
    "\n",
    "rnn1 = Flux.RNNCell( x_dim, h_dim, sigmoid ) #define x_dim and then h_dim\n",
    "m1 = Flux.Dense( 2 => 1 , sigmoid )\n",
    "\n",
    "h , y = rnn1( h_init , x_init ) #pass h and then x to get the h and y outputs\n",
    "\n",
    "h , y = rnn1( h , y ) #pass h and then x to get the h and y outputs\n",
    "h , y = rnn1( h , y ) #pass h and then x to get the h and y outputs\n",
    "h , y = rnn1( h , y ) #pass h and then x to get the h and y outputs\n",
    "y_hat = m1( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Matrix{Float32}}:\n",
       " [0.7193387 0.33868003; 0.016090214 0.256925; 0.2488938 0.47927523]\n",
       " [0.56273735 0.5567441; 0.20874453 0.45442927; 0.6528861 0.7265376]\n",
       " [0.5801285 0.41532242; 0.91156983 0.92337066; 0.66897005 0.5273158]\n",
       " [0.73792315 0.7799565; 0.5407746 0.5092; 0.6379438 0.23041004]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       " -0.344579  -0.125573\n",
       " -0.119549  -0.341085\n",
       "  0.22776    0.012745\n",
       " -0.451451  -0.568937\n",
       "  0.337278  -0.00986344"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       "  0.138712    0.112828\n",
       " -0.326718   -0.57339\n",
       " -0.0753149  -0.490038\n",
       " -0.862643   -0.845221\n",
       "  0.167302    0.156598"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       "  0.477808   0.602143\n",
       " -0.618007  -0.544691\n",
       " -0.528367  -0.617153\n",
       " -0.925458  -0.913587\n",
       "  0.371714   0.403402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×2 Matrix{Float32}:\n",
       "  0.738152   0.738869\n",
       " -0.659737  -0.444979\n",
       " -0.471581  -0.31129\n",
       " -0.907443  -0.83823\n",
       "  0.788016   0.860447"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Nothing}:\n",
       " nothing\n",
       " nothing\n",
       " nothing\n",
       " nothing"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsteps = 4\n",
    "xs = [rand(Float32, 3, 2) for i in 1:nsteps]\n",
    "display( xs )\n",
    "m = Chain(RNN(3, 5))\n",
    "[ display(m(x)) for x = xs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <span style=\"color:orange\">write a function and make a function for the gradient of the function</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
